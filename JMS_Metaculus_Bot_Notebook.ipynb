{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joe-segal/metaculus-bot-forecaster/blob/main/JMS_Metaculus_Bot_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM Forecasting Bot\n",
        "*Created by Kirill and Tom, revised by Ryan*\n",
        "\n",
        "The code below is used for making LLM-powered forecasts in Metaculus' [AI benchmarking competition](https://www.metaculus.com/project/ai-benchmarking-pilot/).\n",
        "\n",
        "Specifically, it does the following:\n",
        "\n",
        "* Gets questions from the project page using the Metaculus API\n",
        "* Gets four separate forecasts from the LLM, three independently and the fourth assessing the reasoning of the first three and producing its own.\n",
        "* Predicts on the Metaculus questions and shares a comment describing the reasoning of the fourth LLM forecast.\n",
        "\n",
        "Features and options:\n",
        "\n",
        "* Allows you to choose whether it repredicts on questions it's already forecasted on or ignores them.\n",
        "* Allows for the use of [Perplexity search](https://www.perplexity.ai/) for additional research, using a prompt formed by an LLM.\n",
        "    * *Previously this allowed for the use of pre-computed Perpelxity results, but this is no longer supported by Metaculus.*\n",
        "* Can be used with an automated workflow via Github actions to monitor the project for new open questions and make forecasts when there are some\n",
        "    * See [this Github repo](https://github.com/ryooan/metaculus-bot-forecaster) for how to set this up"
      ],
      "metadata": {
        "id": "zMlSOSdSYCds"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üö®üö®üö® Warning üö®üö®üö®\n",
        "\n",
        "**You are responsible for monitoring the costs of your implementation, especially if using the automated Github workflow. Cost estimates computed by this notebook are rough estimates only, make sure to check and monitor how much you are spending and the funds in your relevant accounts.**"
      ],
      "metadata": {
        "id": "TZ5X6zf7bYT-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzqR47EYbh6p"
      },
      "source": [
        "## Getting Started\n",
        "\n",
        "### Make a Metaculus Bot Account\n",
        "The first step will be to make a Metaculus bot account. Instructions for how to do this have likely already been provided to you, either via a page on Metaculus or at an event.\n",
        "\n",
        "### Secrets and Tokens\n",
        "\n",
        "You need to set secrets 1) and 2) in order to make forecasts. Secrets 3) and 4) are necessary if you will be using Perplexity research.\n",
        "\n",
        "1) METACULUS_TOKEN (you can find it or create it here - https://www.metaculus.com/admin/authtoken/tokenproxy/, or ask Metaculus to share it with you).\n",
        "\n",
        "2) OPENAPI_API_KEY - (you can find it here https://platform.openai.com/settings/profile?tab=api-keys).\n",
        "\n",
        "3) PERPLEXITY_API_KEY - You can generate an API key here: https://docs.perplexity.ai/docs/getting-started\n",
        "\n",
        "These secrets can be set in your Google colab account using the key on the left side.\n",
        "\n",
        "*See the [Github repo](https://github.com/ryooan/metaculus-bot-forecaster) for special instructions necessary for setting your secrets in Github if you intend to use the automated Github action.*\n",
        "\n",
        "*Note: previously this also used a QUESTIONS_API_KEY which got some precomputed Perplexity results stored by Metaculus, but this is no longer supported by Metaculus.*\n",
        "\n",
        "### Setting Inputs\n",
        "\n",
        "Once your tokens are set correctly, you can proceed to the [Inputs section](https://colab.research.google.com/drive/1_P7_QNJiJyWBY2qCVu2-_8gVPD1X7mX3?authuser=2#scrollTo=6cbruBaVtaZh). That should be the only section most users will need. More advanced users can edit the [Setup](https://colab.research.google.com/drive/1_P7_QNJiJyWBY2qCVu2-_8gVPD1X7mX3?authuser=2#scrollTo=tNl_mbJaX60R) and [Code](https://colab.research.google.com/drive/1_P7_QNJiJyWBY2qCVu2-_8gVPD1X7mX3?authuser=2#scrollTo=k8vtze4SXtR3) sections if desired, but this is not recommended unless you have coding experience."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup\n",
        "\n",
        "It is recommended that you do not edit these cells unless you have coding experience.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "tNl_mbJaX60R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML, display\n",
        "\n",
        "def set_css(*args, **kwargs):\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)\n",
        "\n",
        "#according to this link the above wraps the output text: https://stackoverflow.com/questions/58890109/line-wrapping-in-collaboratory-google-results"
      ],
      "metadata": {
        "id": "girVrSlJkWUL"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "HifodCwcGU0j",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 642
        },
        "outputId": "300bee03-cc88-4a74-ba46-ee6fa84377b9"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.37.0-py3-none-any.whl (337 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m337.0/337.0 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.7.4)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n",
            "Installing collected packages: h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 openai-1.37.0\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.7.4)\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.7.0\n"
          ]
        }
      ],
      "source": [
        "!pip install openai\n",
        "!pip install tiktoken\n",
        "import datetime\n",
        "import json\n",
        "import os\n",
        "import requests\n",
        "import tiktoken\n",
        "import re\n",
        "\n",
        "from openai import OpenAI\n",
        "\n",
        "#use the below to detect if it's being run in google colab, if it's not this skips an error\n",
        "def in_colab():\n",
        "    try:\n",
        "        import google.colab\n",
        "        return True\n",
        "    except ImportError:\n",
        "        return False\n",
        "\n",
        "if in_colab():\n",
        "    from google.colab import userdata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "SoELZnYOGXsV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4f4f9669-42b8-4bf4-9d34-350b23e7fc87"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading secrets from userdata (Google Colab)\n"
          ]
        }
      ],
      "source": [
        "#the below is used to get secrets when using github actions to automate\n",
        "#initialize\n",
        "metaculus_token = None\n",
        "#questions_api_key = None\n",
        "perplexity_api_key = None\n",
        "\n",
        "# Function to load secrets from the specified path\n",
        "def load_secrets(secrets_path):\n",
        "    try:\n",
        "        with open(secrets_path, 'r') as secrets_file:\n",
        "            secrets = json.loads(secrets_file.read())\n",
        "            for k, v in secrets.items():\n",
        "                os.environ[k] = v\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading secrets from {secrets_path}: {e}\")\n",
        "\n",
        "# Main code block\n",
        "try:\n",
        "    if 'secretsPath' in globals():\n",
        "        print(f\"secretsPath exists: {secretsPath}\")\n",
        "        load_secrets(secretsPath)\n",
        "\n",
        "        metaculus_token = os.environ['METACULUS_TOKEN']\n",
        "        #questions_api_key = os.environ['QUESTIONS_API_KEY']\n",
        "        perplexity_api_key = os.environ['PERPLEXITY_API_KEY']\n",
        "    else:\n",
        "        raise NameError(\"secretsPath not defined\")\n",
        "except NameError:\n",
        "    print(\"Loading secrets from userdata (Google Colab)\")\n",
        "    metaculus_token = userdata.get('METACULUS_TOKEN')\n",
        "    os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "    #questions_api_key = userdata.get('QUESTIONS_API_KEY')\n",
        "    perplexity_api_key = userdata.get('PERPLEXITY_API_KEY')\n",
        "except KeyError as e:\n",
        "    print(f\"Missing required environment variable: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inputs\n",
        "\n",
        "The cell below contains all of the main settings that you can change. See comments in the cell for an explanation of each. Modify them as you see fit and then run all of the cells below to forecast.\n",
        "\n",
        "*You can press Ctrl+F10 to run all of the cells after the selected one.*"
      ],
      "metadata": {
        "id": "6cbruBaVtaZh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PROJECT_ID = 3349  # 3129 is ID of AI Becnhmarking Pilot project. We kindly ask you not to forecast on any public tournaments or public questions in general\n",
        "MAX_QUESTIONS_TO_FORECAST = 25  # You can set it to some small number for testing or to 1_000_000 to forecast on all available questions\n",
        "REPREDICT = True # if this is false it won't predict on questions it has previously already predicted on. Set it to true to repredict on all open questions, even if it has made previous predictions.\n",
        "SUBMIT_FORECASTS = True # If set to False - forecast, but don't submit results to Metaculus platform. If set to True - forecast, and submit results to Metaculus platform\n",
        "USE_PERPLEXITY_RECENT = True # If set to true the perplexity search used is one that looks for the most recent news on the subject using a GPT prompt completion informed by the forecasting question.\n",
        "QUESTION_IDS_TO_FORECAST = None # Set to None to disable custom filtering by ID. Set to a list of IDs to only forecast on selected questions, i.e. [24191, 24190, 24189]\n",
        "#QUESTION_IDS_TO_FORECAST = [26390]\n",
        "USE_CONFIDENCE = True\n",
        "#USE_METACULUS_PROXY_ENDPOINT = False\n",
        "OPEN_AI_MODEL = 'gpt-4-turbo' # if it is one model running everything openai\n",
        "PERPLEXITY_MODEL = 'llama-3-sonar-large-32k-online'\n",
        "NUM_FORECASTS_TO_AVERAGE = 3 # 1 to go once through on the above OPEN_AI_MODEL, otherwise use MODELS_TO_RUN in sequence\n",
        "METACULUS_SUPPORTED_MODELS = ['gpt-4o']\n",
        "MODELS_TO_RUN = ['gpt-4o', 'gpt-4', 'gpt-4-turbo']\n",
        "\n",
        "# Use this to run your own question, not pulled from Metaculus\n",
        "ANSWER_MANUAL_QUESTION = False\n",
        "#manual_question = \"Will Pfizer's GLP-1 trial agent danuglipron be approved by the FDA for weight loss by January 1 2028?\"\n",
        "manual_question = \"Will a nuclear weapon be detonated as an act of war by Sept 30, 2024?\"\n",
        "manual_question_dict = {\n",
        "  'id': None,\n",
        "  'title': manual_question,\n",
        "  'description': manual_question,\n",
        "  'resolution_criteria': manual_question,\n",
        "  'fine_print': manual_question\n",
        "  }\n",
        "if ANSWER_MANUAL_QUESTION:\n",
        "  SUBMIT_FORECASTS = False\n",
        "\n",
        "# The forecaster weights are used to produce a weighted average of the forecasts. You can adjust these weights to favor a certain forecaster/prompt more heavily.\n",
        "# Weights should sum to 1.\n",
        "forecaster1_weight = 0.2\n",
        "forecaster2_weight = 0.2\n",
        "forecaster3_weight = 0.2\n",
        "forecaster4_weight = 0.4\n",
        "\n",
        "# Prompts\n",
        "# The prompts used are below, these can be edited to hone your LLM forecasts.\n",
        "# Here is a glossary of the variables that can be inserted and used in the prompts.\n",
        "# [[title]]: This is the question text, what shows up at the top of the Metaculus question page\n",
        "# [[resolution_criteria]]: This is the resolution criteria section of the question, excluding fine print\n",
        "# [[fine_print]]: This is the fine print section of the question.\n",
        "# [[background]]: This is the background section of the resolution criteria.\n",
        "# [[today]]: The current date\n",
        "# [[forecaster1]] through {forecaster3}: These are the LLM outputs of forecasters 1 through 3, currently being used to feed into the input of forecaster 4 for it to assess in its own forecast.\n",
        "# [[summary_report]]: This is research from Perplexity. If USE_PERPLEXITY_RECENT is True, this will be the Perplexity info returned when the output of LLM_question_completion is passed to Perplexity.\n",
        "                  # If USE_PERPLEXITY_RECENT is False and ENABLE_PERPLEXITY_RESEARCH is True, it will return pre-computed Perplexity research on the question stored by Metaculus.\n",
        "                  # If both are false it will not return anything.\n",
        "\n",
        "# The LLM_question_completion prompt is used to ask the LLM what question it should ask Perplexity, if using USE_PERPLEXITY_RECENT\n",
        "prompt_template_get_perplexity_question = \"\"\"\n",
        "You're being asked the following forecasting question:\n",
        "\n",
        "The question is:\n",
        "[[title]]\n",
        "\n",
        "And it has these specific resolution details:\n",
        "[[resolution_criteria]]\n",
        "\n",
        "Fine print:\n",
        "[[fine_print]]\n",
        "\n",
        "To get the latest news that will help you forecast on the question, you need to ask your web search tool one question that would be most valuable to help you forecast\n",
        "on this question. The question should be posed so that the web search tool will provide you with the most recent information, including the latest information on the progress toward\n",
        "the criteria in the forecasting question being met. You don't want to influence it to give more negative or positive spin on the information, so make sure to phrase the question in a neutral, objective wording.  Please complete the sentence below with the most valuable question to ask:\n",
        "\n",
        "\"What is the most recent news available and prior occurrences related to the possibility of, and any indicators or evidence of. . . \"\n",
        "\"\"\"\n",
        "\n",
        "# Prompt 1 is used by forecaster 1, with QUESTION_INFO_TEMPLATE appended to the end\n",
        "prompt_template_v1 = \"\"\"\n",
        "You are a professional forecaster interviewing for a job. The interviewer is also a professional forecaster, with a strong track record of accurate forecasts of the future.\n",
        "They will ask you a question, and your task is to provide the most accurate forecast you can. To do this, you evaluate past data and trends carefully, make use of comparison classes\n",
        "of similar events, take into account base rates about how past events unfolded, and outline the best reasons for and against any particular outcome.\n",
        "You know that great forecasters don't just forecast according to the \"vibe\" of the question and the considerations.\n",
        "Instead, they think about the question in a structured way, recording their reasoning as they go,\n",
        "and they always consider multiple perspectives that usually give different conclusions, which they reason about together.\n",
        "You can't know the future, and the interviewer knows that, so you do not need to hedge your uncertainty, you are simply trying to give\n",
        "the most accurate numbers that will be evaluated when the events later unfold.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Prompt 2 is used by forecaster 2, with QUESTION_INFO_TEMPLATE appended to the end\n",
        "prompt_template_v2 = \"\"\"\n",
        "You are a professional forecaster being tested on your skill. You are being asked a question for which you must provide the most accurate forecast you can.\n",
        "Reason step by step about the best forecast, carefully considering the information you are presented with and the potential that some of the\n",
        "background information and information provided by your research assistant may be out of date or conflicting.\n",
        "You also know that a key forecasting practice is to consider the outside view, thinking carefully about reference classes and base rates and how often similar events\n",
        "have happened in the past.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Prompt 3 is used by forecaster 3, with QUESTION_INFO_TEMPLATE appended to the end\n",
        "prompt_template_v3 = \"\"\"\n",
        "You are a professional forecaster trying your best to produce an accurate forecast for the question below. You have a keen eye for detail, and know that\n",
        "the resolution criteria and fine print of a question often contain important edge cases that should be considered. You also know that rookie forecasters\n",
        "tend to be underconfident, and so your forecasts should be confident and extremize some of the way toward 1% or 99%. You're also aware that forecasting questions tend\n",
        "to be biased, so when forecasting questions asks about things that would be a departure from the norm or from business as usual, those things tend not to happen.\n",
        "You will use that knowledge to make the best forecast you can, and describe your reasoning step by step.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Forecaster 4 uses prompt 4 parts 1 and 2, with QUESTION_INFO_TEMPLATE inserted between part 1 and part 2\n",
        "final_prompt_part1 = \"\"\"\n",
        "You are a professional forecaster trying your best to produce an accurate forecast for the question below.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "final_prompt_part2 = \"\"\"\n",
        "Now that you know what the question asks and some relevant background and research, your job is to make the best forecast you can. You know that examining the reasoning of other\n",
        "forecasters is an excellent way to improve your own forecast. Below I have provided the reasoning from three other forecasters who predicted on the same question.\n",
        "Examine their reasoning and use it to inform your own, using your expertise as a forecaster to assess which reasoning seems strongest and which seems flawed,\n",
        "as well as which reasoning seems to incorporate the most accurate information about base rates and historic reference classes. Construct your own reasoning and forecast,\n",
        "describing your reasoning step by step and incorporating the strongest arguments from the other forecasters in a way that improves your own reasoning. First produce a\n",
        "one sentence summary of the reasoning of each forecaster (repeating the final probability each predicted), then describe your forecast.\n",
        "\n",
        "Forecaster A:\n",
        "[[forecaster1]]\n",
        "\n",
        "Forecaster B:\n",
        "[[forecaster2]]\n",
        "\n",
        "Forecaster C:\n",
        "[[forecaster3]]\n",
        "\"\"\"\n",
        "\n",
        "# QUESTION_INFO_TEMPLATE is used with the above prompts to share the details about the question with the LLM.\n",
        "QUESTION_INFO_TEMPLATE = \"\"\"\n",
        "\n",
        "The question is:\n",
        "[[title]]\n",
        "\n",
        "Here are details about how the outcome of the question will be determined, make sure your forecast is consistent with these:\n",
        "[[resolution_criteria]]\n",
        "\n",
        "Here is the question's fine print that you need to be consistent with in your forecast:\n",
        "[[fine_print]]\n",
        "\n",
        "Here is some background of the question, though note that some of the details may be out of date:\n",
        "[[background]]\n",
        "\n",
        "Your research assistant provides the following information that is likely more up to date:\n",
        "[[summary_report]]\n",
        "\n",
        "Today is [[today]].\n",
        "\n",
        "Describe your reasoning step by step and give your final probability forecast in the last line of your response along with an assessment of your confidence in the accuracy of this forecast, both values as a percentage probability or confidence in exactly this form: \"Probability: XX.X% - Confidence: XX.X%\", where XX.X is a number between 0 and 100, with at most one decimal place.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "2CWVjJ9_tZ6_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "d68e9d54-5b72-4fd8-9d8a-c9cce10faaf4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "k8vtze4SXtR3"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmWSCwDLbhFJ"
      },
      "source": [
        "Getting questions (only binaries)\n",
        "\n",
        "Getting them 10 at a time, you can change offset to \"scroll\" through them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "O9gVXbHIGfOP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "28340ad0-545a-44d3-b019-8c8d70dc46cc"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "url = \"https://www.metaculus.com/api2/questions/\"\n",
        "\n",
        "params = {\n",
        "    \"has_group\": \"false\",\n",
        "    \"order_by\": \"-activity\",\n",
        "    \"forecast_type\": \"binary\",\n",
        "    \"project\": PROJECT_ID,\n",
        "    \"status\": \"open\", # can change this to 'closed' for testing where you're not submitting a forecast, otherwise leave as open\n",
        "    \"type\": \"forecast\",\n",
        "    \"title-and-description-only\": \"true\",\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "yioIDa8UsCuR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "96c7a736-db34-4199-a2ed-43f98a7c79a0"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "def yield_all_questions():\n",
        "  if ANSWER_MANUAL_QUESTION:\n",
        "    yield manual_question_dict\n",
        "    return\n",
        "\n",
        "  limit = 10 # This is a page limit, not question limit\n",
        "  n = 0\n",
        "  new_questions_found = False\n",
        "\n",
        "  while True:\n",
        "    offset = n * limit\n",
        "    response = requests.get(\n",
        "        url,\n",
        "        params={**params, \"limit\": limit, \"offset\": offset},\n",
        "        headers={\"Authorization\": f\"Token {metaculus_token}\"}\n",
        "    )\n",
        "    response.raise_for_status()\n",
        "    questions = response.json().get(\"results\")\n",
        "\n",
        "    # if repredict is true it will skip to the else and predict on all the questions\n",
        "    # if repredict is false it will see if \"my_predictions\" is empty or not for each question, and only predict on questions without a prediction\n",
        "    if not REPREDICT:\n",
        "        for question in questions:\n",
        "            question_id = question['id']\n",
        "\n",
        "            guess_response = requests.get(\n",
        "                f\"{url}{question_id}/\",\n",
        "                headers={\"Authorization\": f\"Token {metaculus_token}\"}\n",
        "            )\n",
        "            guess_response.raise_for_status()\n",
        "\n",
        "            if not guess_response.json().get(\"my_predictions\"):\n",
        "                new_questions_found = True\n",
        "                yield question\n",
        "    else:\n",
        "        new_questions_found = True\n",
        "        yield from questions\n",
        "\n",
        "    if not response.json().get(\"next\"):\n",
        "      break\n",
        "    n += 1\n",
        "\n",
        "  if not new_questions_found:\n",
        "    print(\"No new questions to predict on.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "dXyQLerREJGk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "e2db7688-bf3b-4e21-ef4d-659c03512da5"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "def extract_probability(s):\n",
        "    s = s.replace(\"**\",\"\")\n",
        "    # Use a regular expression to find all numbers followed by a '%'\n",
        "    matches = re.findall(r'Probability[:\\s]+(?:\\*\\*)?[\\s]*(\\d+\\.?\\d*)%', s)\n",
        "    if matches:\n",
        "        # Return the last number found before a '%'\n",
        "        #return int(matches[-1])\n",
        "        return float(matches[-1]), None\n",
        "    else:\n",
        "        # Return None if no number found\n",
        "        return None, None\n",
        "\n",
        "def extract_probability_and_confidence(s):\n",
        "    s = s.replace(\"**\",\"\")\n",
        "    # Use a regular expression to find all numbers followed by a '%'\n",
        "    matches = re.findall(r'Probability[:\\s]+(?:\\*\\*)?[\\s]*(\\d+\\.?\\d*)%', s)\n",
        "    probability = float(matches[-1])\n",
        "    matches = re.findall(r'Confidence[:\\s]+(?:\\*\\*)?[\\s]*(\\d+\\.?\\d*)%', s)\n",
        "    confidence = float(matches[-1])\n",
        "    return probability, confidence"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# this is used to replace the {} keys with [[]], since sometimes the LLM output uses {} when formatting code.\n",
        "def replace_keys(text, key_dict, delimiter='[[', end_delimiter=']]'):\n",
        "    pattern = re.compile(re.escape(delimiter) + '(.*?)' + re.escape(end_delimiter))\n",
        "    def replace(match):\n",
        "        key = match.group(1)\n",
        "        return key_dict.get(key, match.group(0))  # Return the original if key not found\n",
        "    return pattern.sub(replace, text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "ldAJ5OWBW1mA",
        "outputId": "3d81a63b-e1f2-4b64-94d5-d51ef3e79468"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "EoV2KKC8l5im",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "68bdd59f-c5f2-4fed-8bab-b663d298efcb"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "def predict(question_id, prediction_percentage):\n",
        "  prediction_decimal = float(prediction_percentage) / 100.0\n",
        "  url = f\"https://www.metaculus.com/api2/questions/{question_id}/predict/\"\n",
        "  response = requests.post(\n",
        "      url,\n",
        "      json={\n",
        "        \"prediction\": prediction_decimal\n",
        "      },\n",
        "      headers={\"Authorization\": f\"Token {metaculus_token}\"},\n",
        "  )\n",
        "  response.raise_for_status()\n",
        "  print(f\"Successfully predicted {prediction_percentage}% ({prediction_decimal}) on question {question_id}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "AJUGVfHVrms2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "cb8032b5-4358-4b4b-a55c-f2232334ff92"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "def formulate_comment(prediction_json):\n",
        "  comment_blocks = []\n",
        "  if \"reasoning_base_rate\" in prediction_json:\n",
        "    comment_blocks.append(\"## Base rate estimation\")\n",
        "    comment_blocks.append(prediction_json[\"reasoning_base_rate\"])\n",
        "  if \"reasoning_reference_classes\" in prediction_json:\n",
        "    comment_blocks.append(\"## Reference classes\")\n",
        "    comment_blocks.append(prediction_json[\"reasoning_reference_classes\"])\n",
        "  if \"reasoning_other\" in prediction_json:\n",
        "    comment_blocks.append(\"## Additional\")\n",
        "    comment_blocks.append(prediction_json[\"reasoning_other\"])\n",
        "  return \"\\n\".join(comment_blocks) if comment_blocks else \"No reasoning provided\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "7uhKwbzQsKby",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "b6d3f610-9b4b-4494-d75f-683c5f7cfbc2"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "def comment(question_id, comment_text):\n",
        "\n",
        "  # for submit_type choose \"S\" to post regular comment and \"N\" for private. Tournament submissions should be private comments.\n",
        "  url = f\"https://www.metaculus.com/api2/comments/\"\n",
        "  response = requests.post(\n",
        "    url,\n",
        "    json={\n",
        "      \"comment_text\":comment_text,\"submit_type\":\"N\",\"include_latest_prediction\":True,\"question\":question_id\n",
        "    },\n",
        "    headers={\"Authorization\": f\"Token {metaculus_token}\"},\n",
        "  )\n",
        "  response.raise_for_status()\n",
        "  print(\"Comment Success!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def estimate_pricing(input, output, model):\n",
        "  encoding = tiktoken.encoding_for_model(model)\n",
        "  input_len = len(encoding.encode(input))\n",
        "  output_len = len(encoding.encode(output))\n",
        "\n",
        "  print(\"Input text is approx %s tokens.\" % input_len)\n",
        "  print(\"Output text is approx %s tokens.\" % output_len)\n",
        "\n",
        "\n",
        "  # hard coding for now, maybe make it smarter later\n",
        "  # units of $ per token\n",
        "  gpt4o_input_pricing = 5 / 1_000_000\n",
        "  gpt4o_output_pricing = 15 / 1_000_000\n",
        "\n",
        "  input_cost = input_len * gpt4o_input_pricing\n",
        "  output_cost = output_len * gpt4o_output_pricing\n",
        "  total_cost = input_cost + output_cost\n",
        "\n",
        "  return input_cost, output_cost, total_cost"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "THGjV1tPd_7Q",
        "outputId": "5d024d90-1092-45e3-ac2c-3c3a222bb5ca"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def run_llm(today, client, template_values, prompt_template, summary_report, model, looking_for_probability=True):\n",
        "\n",
        "  title = template_values[\"title\"]\n",
        "  resolution_criteria = template_values[\"resolution_criteria\"]\n",
        "  background = template_values[\"description\"]\n",
        "  if template_values[\"fine_print\"]:\n",
        "    fine_print = template_values[\"fine_print\"]\n",
        "  else:\n",
        "    fine_print = \"none\"\n",
        "\n",
        "  prompt_dict = {\n",
        "      \"title\": title,\n",
        "      \"summary_report\": summary_report,\n",
        "      \"today\": today,\n",
        "      \"background\": background,\n",
        "      \"fine_print\": fine_print,\n",
        "      \"resolution_criteria\": resolution_criteria,\n",
        "  }\n",
        "\n",
        "  prompt_text = replace_keys(prompt_template, prompt_dict)\n",
        "\n",
        "  #print(\"Here is the prompt used:\")\n",
        "  #print(\"prompt_text\")\n",
        "  #print(prompt_text)\n",
        "  #print(\"\")\n",
        "\n",
        "  chat_completion = client.chat.completions.create(\n",
        "    model=model,\n",
        "    messages=[\n",
        "      {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": prompt_text\n",
        "      }\n",
        "    ]\n",
        "  )\n",
        "\n",
        "  gpt_text = chat_completion.choices[0].message.content\n",
        "\n",
        "  #estimate cost\n",
        "  input_cost, output_cost, total_cost = estimate_pricing(prompt_text, gpt_text, model)\n",
        "\n",
        "  if looking_for_probability:\n",
        "    # Regular expression to find the number following 'Probability: '\n",
        "    try:\n",
        "      probability_match, confidence_match = extract_probability_and_confidence(gpt_text)\n",
        "    except:\n",
        "      print('Error extracting numbers from text:')\n",
        "      print(gpt_text)\n",
        "      raise ValueError\n",
        "\n",
        "    # Extract the number if a match is found\n",
        "    if probability_match:\n",
        "        #probability = int(probability_match) # int(match.group(1))\n",
        "        probability = float(probability_match)\n",
        "        #print(f\"The extracted probability is: {probability}%\")\n",
        "        #probability = min(max(probability, 3), 97) # To prevent extreme forecasts\n",
        "    else:\n",
        "        probability = None\n",
        "        print(\"No probability found in the text! Skipping!\")\n",
        "        # Extract the number if a match is found\n",
        "    if confidence_match:\n",
        "        #probability = int(probability_match) # int(match.group(1))\n",
        "        confidence = float(confidence_match)\n",
        "        #print(f\"The extracted probability is: {probability}%\")\n",
        "        #probability = min(max(probability, 3), 97) # To prevent extreme forecasts\n",
        "    else:\n",
        "        confidence = None\n",
        "        print(\"No confidence found in the text! Skipping!\")\n",
        "  else:\n",
        "    probability = None\n",
        "    confidence = None\n",
        "  return probability, confidence, gpt_text, input_cost, output_cost, total_cost\n",
        "\n",
        "\n",
        "def run_llm_metaculus(today, template_values, prompt_template, summary_report, model, looking_for_probability=True):\n",
        "\n",
        "  title = template_values[\"title\"]\n",
        "  resolution_criteria = template_values[\"resolution_criteria\"]\n",
        "  background = template_values[\"description\"]\n",
        "  if template_values[\"fine_print\"]:\n",
        "    fine_print = template_values[\"fine_print\"]\n",
        "  else:\n",
        "    fine_print = \"none\"\n",
        "\n",
        "  prompt_dict = {\n",
        "      \"title\": title,\n",
        "      \"summary_report\": summary_report,\n",
        "      \"today\": today,\n",
        "      \"background\": background,\n",
        "      \"fine_print\": fine_print,\n",
        "      \"resolution_criteria\": resolution_criteria,\n",
        "  }\n",
        "\n",
        "  prompt_text = replace_keys(prompt_template, prompt_dict)\n",
        "\n",
        "  #print(\"Here is the prompt used:\")\n",
        "  #print(\"prompt_text\")\n",
        "  #print(prompt_text)\n",
        "  #print(\"\")\n",
        "\n",
        "  url = \"https://www.metaculus.com/proxy/openai/v1/chat/completions\"\n",
        "  auth_token = metaculus_token\n",
        "\n",
        "  # Define the payload as a dictionary\n",
        "  payload = {\n",
        "      \"model\": model,\n",
        "      \"messages\": [\n",
        "          { \"role\": \"user\", \"content\": prompt_text },\n",
        "      ]\n",
        "  }\n",
        "\n",
        "  # Set the headers\n",
        "  headers = {\n",
        "      \"Content-Type\": \"application/json\",\n",
        "      \"Authorization\": f\"Token {auth_token}\"\n",
        "  }\n",
        "\n",
        "  # Make the POST request\n",
        "  response = requests.post(url, headers=headers, data=json.dumps(payload))\n",
        "\n",
        "  # Check if the request was successful\n",
        "  if response.status_code == 200:\n",
        "      print(\"Request was successful!\")\n",
        "\n",
        "      # Parse the response JSON\n",
        "      result = response.json()\n",
        "\n",
        "      # Extract the desired text\n",
        "      gpt_text = result[\"choices\"][0][\"message\"][\"content\"]\n",
        "\n",
        "      # Print the extracted content\n",
        "      #print(gpt_text)\n",
        "\n",
        "  else:\n",
        "      print(f\"Request failed with status code: {response.status_code}\")\n",
        "      print(response.text)\n",
        "      raise ValueError\n",
        "\n",
        "  #estimate cost\n",
        "  input_cost, output_cost, total_cost = estimate_pricing(prompt_text, gpt_text, model)\n",
        "\n",
        "  if looking_for_probability:\n",
        "    # Regular expression to find the number following 'Probability: '\n",
        "    try:\n",
        "      probability_match, confidence_match = extract_probability_and_confidence(gpt_text)\n",
        "    except:\n",
        "      print('Error extracting numbers from text:')\n",
        "      print(gpt_text)\n",
        "      raise ValueError\n",
        "\n",
        "    # Extract the number if a match is found\n",
        "    if probability_match:\n",
        "        #probability = int(probability_match) # int(match.group(1))\n",
        "        probability = float(probability_match)\n",
        "        #print(f\"The extracted probability is: {probability}%\")\n",
        "        #probability = min(max(probability, 3), 97) # To prevent extreme forecasts\n",
        "    else:\n",
        "        probability = None\n",
        "        print(\"No probability found in the text! Skipping!\")\n",
        "        # Extract the number if a match is found\n",
        "    if confidence_match:\n",
        "        #probability = int(probability_match) # int(match.group(1))\n",
        "        confidence = float(confidence_match)\n",
        "        #print(f\"The extracted probability is: {probability}%\")\n",
        "        #probability = min(max(probability, 3), 97) # To prevent extreme forecasts\n",
        "    else:\n",
        "        confidence = None\n",
        "        print(\"No confidence found in the text! Skipping!\")\n",
        "  else:\n",
        "    probability = None\n",
        "    confidence = None\n",
        "  return probability, confidence, gpt_text, input_cost, output_cost, total_cost"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "XK4FbnKJb98Q",
        "outputId": "80607d4c-cbe5-457f-9ed1-4bae4b8a3cfc"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def call_perplexity(perplexity_prompt, perplexity_api_key):\n",
        "\n",
        "  from openai import OpenAI\n",
        "\n",
        "  YOUR_API_KEY = perplexity_api_key\n",
        "\n",
        "  messages = [\n",
        "      {\n",
        "          \"role\": \"system\",\n",
        "          \"content\": (\n",
        "              \"You are an artificial intelligence assistant and you need to \"\n",
        "              \"engage in a helpful, detailed, polite conversation with a user.  \"\n",
        "              \"Please supply relevant objective and balanced information and status regarding the following questions the user will pose to you.  \"\n",
        "              \"You may also supply expert opinions (and cite the source of) if you find some that you think are important to include, \"\n",
        "              \"but you are not to editorialize very much yourself about this information.  \"\n",
        "              \"You are supporting an analysis team who are already experts in this area, \"\n",
        "              \"but need to get a quick yet thorough update on any recent news developments and the overall current status of any key factors relating to this question.  \"\n",
        "              \"Questions will usually be stated in terms something that WILL happen, or that something WILL NOT happen - either way it is asked, be sure to provide recent information on why the event or condition MAY happen and why it MAY NOT happen.\"\n",
        "\n",
        "          ),\n",
        "      },\n",
        "      {\n",
        "          \"role\": \"user\",\n",
        "          \"content\": (\n",
        "              perplexity_prompt\n",
        "          ),\n",
        "      },\n",
        "  ]\n",
        "\n",
        "  perplexity_client = OpenAI(api_key=YOUR_API_KEY, base_url=\"https://api.perplexity.ai\")\n",
        "\n",
        "  # chat completion without streaming\n",
        "  response = perplexity_client.chat.completions.create(\n",
        "      model=PERPLEXITY_MODEL,\n",
        "      messages=messages,\n",
        "  )\n",
        "\n",
        "  content = response.choices[0].message.content\n",
        "\n",
        "  print(\"Generated research from perplexity:\")\n",
        "  print(content)\n",
        "  print(\"\")\n",
        "\n",
        "  # get token and cost estimate\n",
        "\n",
        "  # currently using the GPT tokenizer with a 1.3 multiplier. Hacky and wrong, but rough estimate.\n",
        "  # See here for 1.3 factor estimate source: https://github.com/continuedev/continue/issues/878\n",
        "\n",
        "  perplexity_token_pricing = 1/1_000_000\n",
        "  perplexity_cost_fixed = 5/1_000\n",
        "\n",
        "  multiplier = 1.3\n",
        "  encoding = tiktoken.encoding_for_model(OPEN_AI_MODEL)\n",
        "  input_text = perplexity_prompt\n",
        "  output_text = content\n",
        "  input_len = len(encoding.encode(input_text)) * multiplier\n",
        "  output_len = len(encoding.encode(output_text)) * multiplier\n",
        "\n",
        "  input_cost = input_len * perplexity_token_pricing\n",
        "  output_cost = output_len * perplexity_token_pricing\n",
        "  fixed_cost = perplexity_cost_fixed\n",
        "  total_cost = input_cost + output_cost + perplexity_cost_fixed\n",
        "\n",
        "  print(f\"Total perplexity call cost: ${total_cost}\")\n",
        "  print(\"\")\n",
        "\n",
        "  return content, total_cost"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "gWXOgDDWx7il",
        "outputId": "321fc80a-b880-430d-a6f3-8daa634b153f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "W-wVKhWomV2C",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "c8e198d5-86a1-4593-a207-a4ee76651157"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "def clean_gpt_turbo_markdown(text: str) -> str:\n",
        "  match = re.search(r\"```[\\w]+\\s+(.*?)\\s+```\", text, re.DOTALL)\n",
        "  if match:\n",
        "    cleaned_text = match.group(1).strip()\n",
        "  else:\n",
        "    cleaned_text = text\n",
        "  return cleaned_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwc6I64ScFUz"
      },
      "source": [
        "Execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "3C52dUgSG6Pt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "1f122cbe-8038-4bb3-b4e7-75a5ec97bf63"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'USE_METACULUS_PROXY_ENDPOINT' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-bef24331c34c>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtoday\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%Y-%m-%d\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mUSE_METACULUS_PROXY_ENDPOINT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0;31m#client = OpenAI(base_url='https://www.metaculus.com/proxy/openai/v1/chat/completions')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'USE_METACULUS_PROXY_ENDPOINT' is not defined"
          ]
        }
      ],
      "source": [
        "today = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
        "#if USE_METACULUS_PROXY_ENDPOINT:\n",
        "#  client = OpenAI(base_url='https://www.metaculus.com/proxy/openai/v1/chat/completions')\n",
        "#else:\n",
        "#  client = OpenAI()\n",
        "openai_client = OpenAI()\n",
        "\n",
        "model = OPEN_AI_MODEL\n",
        "# ENABLE_PERPLEXITY_RESEARCH = False  # Previously this could be set to True to get and use pre-computed Perplexity research results for the question, set to False otherwise.\n",
        "# However, Metaculus no longer supports pre-computed Perplexity research, so it has been permanently set to False here to skip over that setp.\n",
        "\n",
        "prompt_templates = [prompt_template_v1 + QUESTION_INFO_TEMPLATE, prompt_template_v2 + QUESTION_INFO_TEMPLATE, prompt_template_v3 + QUESTION_INFO_TEMPLATE]\n",
        "\n",
        "forecasted_count = 0\n",
        "\n",
        "for question_to_forecast_dict in yield_all_questions():\n",
        "  if forecasted_count >= MAX_QUESTIONS_TO_FORECAST:\n",
        "    break\n",
        "  if QUESTION_IDS_TO_FORECAST is not None and question_to_forecast_dict[\"id\"] not in QUESTION_IDS_TO_FORECAST:\n",
        "    continue\n",
        "\n",
        "  final_forecasts = []\n",
        "\n",
        "  for q in range(0, NUM_FORECASTS_TO_AVERAGE):\n",
        "\n",
        "    print(\"Now Forecasting Question:\")\n",
        "    print(question_to_forecast_dict[\"id\"], question_to_forecast_dict[\"title\"])\n",
        "    print(\"\")\n",
        "    print(\"Run %s\" % (q+1))\n",
        "    print(\"\")\n",
        "\n",
        "    if NUM_FORECASTS_TO_AVERAGE>1:\n",
        "      model = MODELS_TO_RUN[q % len(MODELS_TO_RUN)]\n",
        "      if model in METACULUS_SUPPORTED_MODELS:\n",
        "        print(\"Using Metaculus endpoint for model: %s\" % model)\n",
        "        USE_METACULUS_PROXY_ENDPOINT = True\n",
        "      else:\n",
        "        print(\"Using OpenAI endpoint for model: %s\" % model)\n",
        "        USE_METACULUS_PROXY_ENDPOINT = False\n",
        "\n",
        "    #define perplexity research to use\n",
        "    perplexity_total_cost = \"N/A\"\n",
        "\n",
        "    #if ENABLE_PERPLEXITY_RESEARCH:\n",
        "    #  summary_report = get_perplexity_research(question_to_forecast[\"id\"])\n",
        "    #else:\n",
        "    #  #summary_report = \"No results found, please use your own knowledge and judgement to forecast\"\n",
        "    #  summary_report = \"\"\n",
        "    summary_report = \"\" # ?\n",
        "\n",
        "    # set summary_report to the perplexity recent search if enabled\n",
        "    if USE_PERPLEXITY_RECENT:\n",
        "      print(\"Getting Perplexity recent data...\")\n",
        "      print (\"\")\n",
        "      #print(\"run_llm(...):\")\n",
        "      #print(\"today\", today)\n",
        "      #print(\"client\", client)\n",
        "      #print(\"question_to_forecast_dict\", question_to_forecast_dict)\n",
        "      #print(\"prompt_template_get_perplexity_question\", prompt_template_get_perplexity_question)\n",
        "      #print(\"summary_report\", summary_report)\n",
        "      #print(\"model\", model)\n",
        "\n",
        "      #get prompt completion for use with perplexity\n",
        "      #probability, confidence, perplexity_recent_prompt_completion, completion_input_cost, completion_output_cost, completion_total_cost = run_llm(today, client, question_to_forecast_dict, prompt_template_get_perplexity_question, summary_report, model, looking_for_probability=False)\n",
        "      if USE_METACULUS_PROXY_ENDPOINT:\n",
        "        probability, confidence, perplexity_recent_prompt_completion, completion_input_cost, completion_output_cost, completion_total_cost = run_llm_metaculus(today, question_to_forecast_dict, prompt_template_get_perplexity_question, summary_report, model, looking_for_probability=False)\n",
        "      else:\n",
        "        probability, confidence, perplexity_recent_prompt_completion, completion_input_cost, completion_output_cost, completion_total_cost = run_llm(today, openai_client, question_to_forecast_dict, prompt_template_get_perplexity_question, summary_report, model, looking_for_probability=False)\n",
        "      #print(\"perplexity_recent_prompt_completion\", perplexity_recent_prompt_completion)\n",
        "      #print(\"completion_input_cost\", completion_input_cost)\n",
        "      #print(\"completion_output_cost\", completion_output_cost)\n",
        "      #print(\"completion_total_cost\", completion_total_cost)\n",
        "      #print(\"\")\n",
        "\n",
        "      perplexity_recent_prompt = perplexity_recent_prompt_completion if (\"What is the most recent news available and prior occurrences related to\".lower() in perplexity_recent_prompt_completion.lower()) else (\"What is the most recent news available and prior occurrences related to the possibility of, and any indicators or evidence of \" + perplexity_recent_prompt_completion)\n",
        "\n",
        "      print(f\"The completed question posed to perplexity reads: {perplexity_recent_prompt}\")\n",
        "      #get recent news from perplexity\n",
        "      #print(\"call_perplexity(...)\")\n",
        "      perplexity_content, perplexity_cost = call_perplexity(perplexity_recent_prompt, perplexity_api_key)\n",
        "\n",
        "      summary_report = perplexity_content\n",
        "      perplexity_total_cost = completion_total_cost + perplexity_cost\n",
        "\n",
        "    # need to iterate through prompts here\n",
        "    all_forecasts = []\n",
        "    overall_cost = 0\n",
        "\n",
        "    i = 0\n",
        "    for prompt_template in prompt_templates:\n",
        "      i+=1\n",
        "      print(\"Running initial prompt %s\" % i)\n",
        "      #print(\"run_llm(...)\")\n",
        "      if USE_METACULUS_PROXY_ENDPOINT:\n",
        "        try: # i'm letting it possibly run twice, in case it fails the first time due to returning the numerical result in a weird way\n",
        "          probability, confidence, gpt_text, input_cost, output_cost, total_cost = run_llm_metaculus(today, question_to_forecast_dict, prompt_template, summary_report, model)\n",
        "        except:\n",
        "          probability, confidence, gpt_text, input_cost, output_cost, total_cost = run_llm_metaculus(today, question_to_forecast_dict, prompt_template, summary_report, model)\n",
        "      else:\n",
        "        try:\n",
        "          probability, confidence, gpt_text, input_cost, output_cost, total_cost = run_llm(today, openai_client, question_to_forecast_dict, prompt_template, summary_report, model)\n",
        "        except:\n",
        "          probability, confidence, gpt_text, input_cost, output_cost, total_cost = run_llm(today, openai_client, question_to_forecast_dict, prompt_template, summary_report, model)\n",
        "      all_forecasts.append((probability, confidence, gpt_text, input_cost, output_cost, total_cost))\n",
        "      overall_cost += total_cost\n",
        "      print(f\"Output Reasoning:\")\n",
        "      print(gpt_text)\n",
        "      print(\"\")\n",
        "      print(\"Extracted probability percentage value (0-100): %s\" % probability)\n",
        "      #print(f\"Input cost: ${input_cost}\")\n",
        "      #print(f\"Output cost: ${output_cost}\")\n",
        "      #print(f\"Total cost: ${total_cost}\")\n",
        "      #print(f\"Perplexity search costs (including LLM prompt completion): ${completion_total_cost + perplexity_cost}\")\n",
        "      print(\"\")\n",
        "      print(\"~~~~ NEXT PROMPT ~~~~\")\n",
        "      print(\"\")\n",
        "\n",
        "    final_prompt_part2_dict = {\n",
        "        \"forecaster1\": all_forecasts[0][2],\n",
        "        \"forecaster2\": all_forecasts[1][2],\n",
        "        \"forecaster3\": all_forecasts[2][2],\n",
        "    }\n",
        "\n",
        "    formatted_final_prompt_part2 = replace_keys(final_prompt_part2, final_prompt_part2_dict)\n",
        "\n",
        "    print(\"+++++++++++ FINAL PROMPT (4) +++++++++++++++++\")\n",
        "\n",
        "    final_prompt_template = final_prompt_part1 + QUESTION_INFO_TEMPLATE + formatted_final_prompt_part2\n",
        "\n",
        "    print(\"Running final prompt...\")\n",
        "\n",
        "    if USE_METACULUS_PROXY_ENDPOINT:\n",
        "      try: # i'm letting it possibly run twice, in case it fails the first time due to returning the numerical result in a weird way\n",
        "        probability, confidence, gpt_text, input_cost, output_cost, total_cost = run_llm_metaculus(today, question_to_forecast_dict, prompt_template, summary_report, model)\n",
        "      except:\n",
        "        probability, confidence, gpt_text, input_cost, output_cost, total_cost = run_llm_metaculus(today, question_to_forecast_dict, prompt_template, summary_report, model)\n",
        "    else:\n",
        "      try:\n",
        "        probability, confidence, gpt_text, input_cost, output_cost, total_cost = run_llm(today, openai_client, question_to_forecast_dict, prompt_template, summary_report, model)\n",
        "      except:\n",
        "        probability, confidence, gpt_text, input_cost, output_cost, total_cost = run_llm(today, openai_client, question_to_forecast_dict, prompt_template, summary_report, model)\n",
        "    #print(\"\")\n",
        "    #print(gpt_text)\n",
        "    print(\"\")\n",
        "\n",
        "    print(\"Final prompt's individual (non-weighted) extracted probability percentage value (0-100): %s\" % probability)\n",
        "    print(\"\")\n",
        "\n",
        "    if USE_CONFIDENCE:\n",
        "      total_confidence = 0\n",
        "      for forecast in all_forecasts:\n",
        "        total_confidence += forecast[1]\n",
        "      total_confidence += 2.0*confidence\n",
        "      forecaster1_weight = round(all_forecasts[0][1]/total_confidence,4)\n",
        "      forecaster2_weight = round(all_forecasts[1][1]/total_confidence,4)\n",
        "      forecaster3_weight = round(all_forecasts[2][1]/total_confidence,4)\n",
        "      forecaster4_weight = round(2.0*confidence/total_confidence,4)\n",
        "\n",
        "    else:\n",
        "      forecaster1_weight = 0.2\n",
        "      forecaster2_weight = 0.2\n",
        "      forecaster3_weight = 0.2\n",
        "      forecaster4_weight = 0.4\n",
        "\n",
        "    weighted_forecast = forecaster1_weight*float(all_forecasts[0][0]) + forecaster2_weight*float(all_forecasts[1][0]) + forecaster3_weight*float(all_forecasts[2][0]) + forecaster4_weight*float(probability)\n",
        "    weighted_forecast = round(weighted_forecast,1)\n",
        "    overall_cost = overall_cost + total_cost\n",
        "\n",
        "    #create summary strings for comments:\n",
        "    header_string = f\"\"\"\n",
        "    *This forecast is produced from several prompts initially making forecasts, then a summary forecaster looks over all the previous forecasts and rationales and assess them for level of convincingness before giving its own forecast.  Then a weighted forecast is produced by combining all the inidividual and summary forecasts, weighted based on the initial forecasters' reported confidence and an independently assessed level of rationale strength.  The individual forecasts are reasoned based on knowledge stored inside the latest OpenAI GPT models, supplemented with recent information from targeted Perplexity search results.*\n",
        "\n",
        "    * *Main LLM Model used: {model}*\n",
        "    * *Weighted formula: ({forecaster1_weight})({all_forecasts[0][0]}% [Forecaster A]) + ({forecaster2_weight})({all_forecasts[1][0]}% [Forecaster B]) + ({forecaster3_weight})({all_forecasts[2][0]}% [Forecaster C]) + ({forecaster4_weight})({probability}% [Summary Forecaster])*\n",
        "    * *** FINAL WEIGHTED FORECAST: {weighted_forecast}%***\n",
        "\n",
        "    ---\n",
        "\n",
        "    \"\"\"\n",
        "    print('Rationale to submit:')\n",
        "    print(\"\")\n",
        "    print(header_string)\n",
        "    print(\"\")\n",
        "\n",
        "    if SUBMIT_FORECASTS and weighted_forecast is not None:\n",
        "      predict(question_to_forecast_dict[\"id\"], float(weighted_forecast))\n",
        "      comment(question_to_forecast_dict[\"id\"], header_string + \"PERPLEXITY INFO\\n\\n\" + summary_report + \"\\n\\n---\\n\\n\" + \"SUMMARY FORECASTER RATIONALE\\n\\n\" + gpt_text)\n",
        "    final_forecasts.append(float(weighted_forecast))\n",
        "\n",
        "    print(f\"Output Reasoning:\")\n",
        "    print(gpt_text)\n",
        "    print(\"\")\n",
        "    print(\"FINAL WEIGHTED FORECAST:\")\n",
        "    print(f\"{weighted_forecast}%\")\n",
        "    print(\"\")\n",
        "    print(f\"Overall cost was: ${overall_cost}\")\n",
        "    print(\"\")\n",
        "    print(\"################ NEXT QUESTION #################\")\n",
        "    print(\"\")\n",
        "\n",
        "  if NUM_FORECASTS_TO_AVERAGE>1:\n",
        "    #average the forecasts and make a final one\n",
        "    assert len(final_forecasts) == NUM_FORECASTS_TO_AVERAGE\n",
        "    final_forecast = round(sum(final_forecasts)/len(final_forecasts),1)\n",
        "    final_comment = \"Averaging %s independently generated forecasts (%s) generated on different main LLM models (%s) to get a final forecast value of **%s%%**.\" % (NUM_FORECASTS_TO_AVERAGE, \", \".join([(\"%s%%\" % f) for f in final_forecasts]), \", \".join(MODELS_TO_RUN), final_forecast)\n",
        "    print(\"AVERAGED FINAL FORECAST: %s%%\" % final_forecast)\n",
        "    print(\"FINAL COMMENT:\")\n",
        "    print(final_comment)\n",
        "    print(\"\")\n",
        "    if SUBMIT_FORECASTS:\n",
        "      predict(question_to_forecast_dict[\"id\"], float(final_forecast))\n",
        "      comment(question_to_forecast_dict[\"id\"], final_comment)\n",
        "    print(\"################ NEXT QUESTION #################\")\n",
        "    print(\"\")\n",
        "\n",
        "\n",
        "  forecasted_count += 1\n",
        "\n",
        "print(\"Finished forecasting on %s questions.\" % forecasted_count)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6qfSjn0L_8I3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}