{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joe-segal/metaculus-bot-forecaster/blob/main/JMS_Metaculus_Bot_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM Forecasting Bot\n",
        "*Created by Kirill and Tom, revised by Ryan*\n",
        "\n",
        "The code below is used for making LLM-powered forecasts in Metaculus' [AI benchmarking competition](https://www.metaculus.com/project/ai-benchmarking-pilot/).\n",
        "\n",
        "Specifically, it does the following:\n",
        "\n",
        "* Gets questions from the project page using the Metaculus API\n",
        "* Gets four separate forecasts from the LLM, three independently and the fourth assessing the reasoning of the first three and producing its own.\n",
        "* Predicts on the Metaculus questions and shares a comment describing the reasoning of the fourth LLM forecast.\n",
        "\n",
        "Features and options:\n",
        "\n",
        "* Allows you to choose whether it repredicts on questions it's already forecasted on or ignores them.\n",
        "* Allows for the use of [Perplexity search](https://www.perplexity.ai/) for additional research, using a prompt formed by an LLM.\n",
        "    * *Previously this allowed for the use of pre-computed Perpelxity results, but this is no longer supported by Metaculus.*\n",
        "* Can be used with an automated workflow via Github actions to monitor the project for new open questions and make forecasts when there are some\n",
        "    * See [this Github repo](https://github.com/ryooan/metaculus-bot-forecaster) for how to set this up"
      ],
      "metadata": {
        "id": "zMlSOSdSYCds"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ðŸš¨ðŸš¨ðŸš¨ Warning ðŸš¨ðŸš¨ðŸš¨\n",
        "\n",
        "**You are responsible for monitoring the costs of your implementation, especially if using the automated Github workflow. Cost estimates computed by this notebook are rough estimates only, make sure to check and monitor how much you are spending and the funds in your relevant accounts.**"
      ],
      "metadata": {
        "id": "TZ5X6zf7bYT-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzqR47EYbh6p"
      },
      "source": [
        "## Getting Started\n",
        "\n",
        "### Make a Metaculus Bot Account\n",
        "The first step will be to make a Metaculus bot account. Instructions for how to do this have likely already been provided to you, either via a page on Metaculus or at an event.\n",
        "\n",
        "### Secrets and Tokens\n",
        "\n",
        "You need to set secrets 1) and 2) in order to make forecasts. Secrets 3) and 4) are necessary if you will be using Perplexity research.\n",
        "\n",
        "1) METACULUS_TOKEN (you can find it or create it here - https://www.metaculus.com/admin/authtoken/tokenproxy/, or ask Metaculus to share it with you).\n",
        "\n",
        "2) OPENAPI_API_KEY - (you can find it here https://platform.openai.com/settings/profile?tab=api-keys).\n",
        "\n",
        "3) PERPLEXITY_API_KEY - You can generate an API key here: https://docs.perplexity.ai/docs/getting-started\n",
        "\n",
        "These secrets can be set in your Google colab account using the key on the left side.\n",
        "\n",
        "*See the [Github repo](https://github.com/ryooan/metaculus-bot-forecaster) for special instructions necessary for setting your secrets in Github if you intend to use the automated Github action.*\n",
        "\n",
        "*Note: previously this also used a QUESTIONS_API_KEY which got some precomputed Perplexity results stored by Metaculus, but this is no longer supported by Metaculus.*\n",
        "\n",
        "### Setting Inputs\n",
        "\n",
        "Once your tokens are set correctly, you can proceed to the [Inputs section](https://colab.research.google.com/drive/1_P7_QNJiJyWBY2qCVu2-_8gVPD1X7mX3?authuser=2#scrollTo=6cbruBaVtaZh). That should be the only section most users will need. More advanced users can edit the [Setup](https://colab.research.google.com/drive/1_P7_QNJiJyWBY2qCVu2-_8gVPD1X7mX3?authuser=2#scrollTo=tNl_mbJaX60R) and [Code](https://colab.research.google.com/drive/1_P7_QNJiJyWBY2qCVu2-_8gVPD1X7mX3?authuser=2#scrollTo=k8vtze4SXtR3) sections if desired, but this is not recommended unless you have coding experience."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup\n",
        "\n",
        "It is recommended that you do not edit these cells unless you have coding experience.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "tNl_mbJaX60R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML, display\n",
        "\n",
        "def set_css(*args, **kwargs):\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)\n",
        "\n",
        "#according to this link the above wraps the output text: https://stackoverflow.com/questions/58890109/line-wrapping-in-collaboratory-google-results"
      ],
      "metadata": {
        "id": "girVrSlJkWUL",
        "outputId": "296dded9-4e4f-47ae-d097-afbe02e4e67e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "HifodCwcGU0j",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "9d7a6226-398b-4ab7-d4f3-6a6289705966"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.36.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.7.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.7.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.7.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install openai\n",
        "!pip install tiktoken\n",
        "import datetime\n",
        "import json\n",
        "import os\n",
        "import requests\n",
        "import tiktoken\n",
        "import re\n",
        "\n",
        "from openai import OpenAI\n",
        "\n",
        "#use the below to detect if it's being run in google colab, if it's not this skips an error\n",
        "def in_colab():\n",
        "    try:\n",
        "        import google.colab\n",
        "        return True\n",
        "    except ImportError:\n",
        "        return False\n",
        "\n",
        "if in_colab():\n",
        "    from google.colab import userdata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "SoELZnYOGXsV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b1b3e533-3b00-42ec-b991-3fc5c96e9e4b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading secrets from userdata (Google Colab)\n"
          ]
        }
      ],
      "source": [
        "#the below is used to get secrets when using github actions to automate\n",
        "#initialize\n",
        "token = None\n",
        "#questions_api_key = None\n",
        "perplexity_api_key = None\n",
        "\n",
        "# Function to load secrets from the specified path\n",
        "def load_secrets(secrets_path):\n",
        "    try:\n",
        "        with open(secrets_path, 'r') as secrets_file:\n",
        "            secrets = json.loads(secrets_file.read())\n",
        "            for k, v in secrets.items():\n",
        "                os.environ[k] = v\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading secrets from {secrets_path}: {e}\")\n",
        "\n",
        "# Main code block\n",
        "try:\n",
        "    if 'secretsPath' in globals():\n",
        "        print(f\"secretsPath exists: {secretsPath}\")\n",
        "        load_secrets(secretsPath)\n",
        "\n",
        "        token = os.environ['METACULUS_TOKEN']\n",
        "        #questions_api_key = os.environ['QUESTIONS_API_KEY']\n",
        "        perplexity_api_key = os.environ['PERPLEXITY_API_KEY']\n",
        "    else:\n",
        "        raise NameError(\"secretsPath not defined\")\n",
        "except NameError:\n",
        "    print(\"Loading secrets from userdata (Google Colab)\")\n",
        "    token = userdata.get('METACULUS_TOKEN')\n",
        "    os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "    #questions_api_key = userdata.get('QUESTIONS_API_KEY')\n",
        "    perplexity_api_key = userdata.get('PERPLEXITY_API_KEY')\n",
        "except KeyError as e:\n",
        "    print(f\"Missing required environment variable: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inputs\n",
        "\n",
        "The cell below contains all of the main settings that you can change. See comments in the cell for an explanation of each. Modify them as you see fit and then run all of the cells below to forecast.\n",
        "\n",
        "*You can press Ctrl+F10 to run all of the cells after the selected one.*"
      ],
      "metadata": {
        "id": "6cbruBaVtaZh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PROJECT_ID = 3349  # 3129 is ID of AI Becnhmarking Pilot project. We kindly ask you not to forecast on any public tournaments or public questions in general\n",
        "MAX_QUESTIONS_TO_FORECAST = 25  # You can set it to some small number for testing or to 1_000_000 to forecast on all available questions\n",
        "REPREDICT = False # if this is false it won't predict on questions it has previously already predicted on. Set it to true to repredict on all open questions, even if it has made previous predictions.\n",
        "SUBMIT_FORECASTS = True # If set to False - forecast, but don't submit results to Metaculus platform. If set to True - forecast, and submit results to Metaculus platform\n",
        "USE_PERPLEXITY_RECENT = True # If set to true the perplexity search used is one that looks for the most recent news on the subject using a GPT prompt completion informed by the forecasting question.\n",
        "QUESTION_IDS_TO_FORECAST = None # Set to None to disable custom filtering by ID. Set to a list of IDs to only forecast on selected questions, i.e. [24191, 24190, 24189]\n",
        "#QUESTION_IDS_TO_FORECAST = [26233, 26234]\n",
        "\n",
        "# Use this to run your own question, not pulled from Metaculus\n",
        "ANSWER_MANUAL_QUESTION = False\n",
        "#manual_question = \"Will Pfizer's GLP-1 trial agent danuglipron be approved by the FDA for weight loss by January 1 2028?\"\n",
        "manual_question = \"Will a nuclear weapon be detonated as an act of war by Sept 30, 2024?\"\n",
        "manual_question_dict = {\n",
        "  'id': None,\n",
        "  'title': manual_question,\n",
        "  'description': manual_question,\n",
        "  'resolution_criteria': manual_question,\n",
        "  'fine_print': manual_question\n",
        "  }\n",
        "if ANSWER_MANUAL_QUESTION:\n",
        "  SUBMIT_FORECASTS = False\n",
        "\n",
        "OPEN_AI_MODEL = 'gpt-4'\n",
        "PERPLEXITY_MODEL = 'llama-3-sonar-large-32k-online'\n",
        "\n",
        "# The forecaster weights are used to produce a weighted average of the forecasts. You can adjust these weights to favor a certain forecaster/prompt more heavily.\n",
        "# Weights should sum to 1.\n",
        "forecaster1_weight = 0.2\n",
        "forecaster2_weight = 0.2\n",
        "forecaster3_weight = 0.2\n",
        "forecaster4_weight = 0.4\n",
        "\n",
        "# Prompts\n",
        "# The prompts used are below, these can be edited to hone your LLM forecasts.\n",
        "# Here is a glossary of the variables that can be inserted and used in the prompts.\n",
        "# [[title]]: This is the question text, what shows up at the top of the Metaculus question page\n",
        "# [[resolution_criteria]]: This is the resolution criteria section of the question, excluding fine print\n",
        "# [[fine_print]]: This is the fine print section of the question.\n",
        "# [[background]]: This is the background section of the resolution criteria.\n",
        "# [[today]]: The current date\n",
        "# [[forecaster1]] through {forecaster3}: These are the LLM outputs of forecasters 1 through 3, currently being used to feed into the input of forecaster 4 for it to assess in its own forecast.\n",
        "# [[summary_report]]: This is research from Perplexity. If USE_PERPLEXITY_RECENT is True, this will be the Perplexity info returned when the output of LLM_question_completion is passed to Perplexity.\n",
        "                  # If USE_PERPLEXITY_RECENT is False and ENABLE_PERPLEXITY_RESEARCH is True, it will return pre-computed Perplexity research on the question stored by Metaculus.\n",
        "                  # If both are false it will not return anything.\n",
        "\n",
        "# The LLM_question_completion prompt is used to ask the LLM what question it should ask Perplexity, if using USE_PERPLEXITY_RECENT\n",
        "prompt_template_get_perplexity_question = \"\"\"\n",
        "You're being asked the following forecasting question:\n",
        "\n",
        "The question is:\n",
        "[[title]]\n",
        "\n",
        "And it has these specific resolution details:\n",
        "[[resolution_criteria]]\n",
        "\n",
        "Fine print:\n",
        "[[fine_print]]\n",
        "\n",
        "To get the latest news that will help you forecast on the question, you need to ask your web search tool one question that would be most valuable to help you forecast\n",
        "on this question. The question should be posed so that the web search tool will provide you with the most recent information, including the latest information on the progress toward\n",
        "the criteria in the forecasting question being met. Please complete the sentence below with the most valuable question to ask:\n",
        "\n",
        "\"What is the most recent news available and prior occurrences related to. . . \"\n",
        "\"\"\"\n",
        "\n",
        "# Prompt 1 is used by forecaster 1, with QUESTION_INFO_TEMPLATE appended to the end\n",
        "prompt_template_v1 = \"\"\"\n",
        "You are a professional forecaster interviewing for a job. The interviewer is also a professional forecaster, with a strong track record of accurate forecasts of the future.\n",
        "They will ask you a question, and your task is to provide the most accurate forecast you can. To do this, you evaluate past data and trends carefully, make use of comparison classes\n",
        "of similar events, take into account base rates about how past events unfolded, and outline the best reasons for and against any particular outcome.\n",
        "You know that great forecasters don't just forecast according to the \"vibe\" of the question and the considerations.\n",
        "Instead, they think about the question in a structured way, recording their reasoning as they go,\n",
        "and they always consider multiple perspectives that usually give different conclusions, which they reason about together.\n",
        "You can't know the future, and the interviewer knows that, so you do not need to hedge your uncertainty, you are simply trying to give\n",
        "the most accurate numbers that will be evaluated when the events later unfold.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Prompt 2 is used by forecaster 2, with QUESTION_INFO_TEMPLATE appended to the end\n",
        "prompt_template_v2 = \"\"\"\n",
        "You are a professional forecaster being tested on your skill. You are being asked a question for which you must provide the most accurate forecast you can.\n",
        "Reason step by step about the best forecast, carefully considering the information you are presented with and the potential that some of the\n",
        "background information and information provided by your research assistant may be out of date or conflicting.\n",
        "You also know that a key forecasting practice is to consider the outside view, thinking carefully about reference classes and base rates and how often similar events\n",
        "have happened in the past.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Prompt 3 is used by forecaster 3, with QUESTION_INFO_TEMPLATE appended to the end\n",
        "prompt_template_v3 = \"\"\"\n",
        "You are a professional forecaster trying your best to produce an accurate forecast for the question below. You have a keen eye for detail, and know that\n",
        "the resolution criteria and fine print of a question often contain important edge cases that should be considered. You also know that rookie forecasters\n",
        "tend to be underconfident, and so your forecasts should be confident and extremize some of the way toward 1% or 99%. You're also aware that forecasting questions tend\n",
        "to be biased, so when forecasting questions asks about things that would be a departure from the norm or from business as usual, those things tend not to happen.\n",
        "You will use that knowledge to make the best forecast you can, and describe your reasoning step by step.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Forecaster 4 uses prompt 4 parts 1 and 2, with QUESTION_INFO_TEMPLATE inserted between part 1 and part 2\n",
        "final_prompt_part1 = \"\"\"\n",
        "You are a professional forecaster trying your best to produce an accurate forecast for the question below.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "final_prompt_part2 = \"\"\"\n",
        "Now that you know what the question asks and some relevant background and research, your job is to make the best forecast you can. You know that examining the reasoning of other\n",
        "forecasters is an excellent way to improve your own forecast. Below I have provided the reasoning from three other forecasters who predicted on the same question.\n",
        "Examine their reasoning and use it to inform your own, using your expertise as a forecaster to assess which reasoning seems strongest and which seems flawed,\n",
        "as well as which reasoning seems to incorporate the most accurate information about base rates and historic reference classes. Construct your own reasoning and forecast,\n",
        "describing your reasoning step by step and incorporating the strongest arguments from the other forecasters in a way that improves your own reasoning. First produce a\n",
        "one sentence summary of the reasoning of each forecaster (repeating the final probability each predicted), then describe your forecast.\n",
        "\n",
        "Forecaster A:\n",
        "[[forecaster1]]\n",
        "\n",
        "Forecaster B:\n",
        "[[forecaster2]]\n",
        "\n",
        "Forecaster C:\n",
        "[[forecaster3]]\n",
        "\"\"\"\n",
        "\n",
        "# QUESTION_INFO_TEMPLATE is used with the above prompts to share the details about the question with the LLM.\n",
        "QUESTION_INFO_TEMPLATE = \"\"\"\n",
        "\n",
        "The question is:\n",
        "[[title]]\n",
        "\n",
        "Here are details about how the outcome of the question will be determined, make sure your forecast is consistent with these:\n",
        "[[resolution_criteria]]\n",
        "\n",
        "Here is the question's fine print that you need to be consistent with in your forecast:\n",
        "[[fine_print]]\n",
        "\n",
        "Here is some background of the question, though note that some of the details may be out of date:\n",
        "[[background]]\n",
        "\n",
        "Your research assistant provides the following information that is likely more up to date:\n",
        "[[summary_report]]\n",
        "\n",
        "Today is [[today]].\n",
        "\n",
        "Describe your reasoning step by step and give your final probability forecast in the last line of your response, as a percentage probability in exactly this form: \"Probability: XY.Z%\", where XY.Z is a number between 0 and 100, with at most one decimal place.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "2CWVjJ9_tZ6_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "77d8f7c4-c054-4656-e439-5f9457247901"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code\n",
        "\n",
        "It is recommended that you do not edit the below unless you have coding experience.\n",
        "\n",
        "Note that currently the LLM is set to gpt-4o, and this is specified in the code below. This can be changed by advanced users, though changing between OpenAI models is much simpler than changing to a model from a different AI organizations.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "k8vtze4SXtR3"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmWSCwDLbhFJ"
      },
      "source": [
        "Getting questions (only binaries)\n",
        "\n",
        "Getting them 10 at a time, you can change offset to \"scroll\" through them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "O9gVXbHIGfOP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "efb975b5-41df-4e34-9784-5bf27d98abd0"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "url = \"https://www.metaculus.com/api2/questions/\"\n",
        "\n",
        "params = {\n",
        "    \"has_group\": \"false\",\n",
        "    \"order_by\": \"-activity\",\n",
        "    \"forecast_type\": \"binary\",\n",
        "    \"project\": PROJECT_ID,\n",
        "    \"status\": \"open\", # can change this to 'closed' for testing where you're not submitting a forecast, otherwise leave as open\n",
        "    \"type\": \"forecast\",\n",
        "    \"title-and-description-only\": \"true\",\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "yioIDa8UsCuR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "80a54a3b-c756-4722-d9f7-0c1dd4eea3aa"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "def yield_all_questions():\n",
        "  if ANSWER_MANUAL_QUESTION:\n",
        "    yield manual_question_dict\n",
        "    return\n",
        "\n",
        "  limit = 10 # This is a page limit, not question limit\n",
        "  n = 0\n",
        "  new_questions_found = False\n",
        "\n",
        "  while True:\n",
        "    offset = n * limit\n",
        "    response = requests.get(\n",
        "        url,\n",
        "        params={**params, \"limit\": limit, \"offset\": offset},\n",
        "        headers={\"Authorization\": f\"Token {token}\"}\n",
        "    )\n",
        "    response.raise_for_status()\n",
        "    questions = response.json().get(\"results\")\n",
        "\n",
        "    # if repredict is true it will skip to the else and predict on all the questions\n",
        "    # if repredict is false it will see if \"my_predictions\" is empty or not for each question, and only predict on questions without a prediction\n",
        "    if not REPREDICT:\n",
        "        for question in questions:\n",
        "            question_id = question['id']\n",
        "\n",
        "            guess_response = requests.get(\n",
        "                f\"{url}{question_id}/\",\n",
        "                headers={\"Authorization\": f\"Token {token}\"}\n",
        "            )\n",
        "            guess_response.raise_for_status()\n",
        "\n",
        "            if not guess_response.json().get(\"my_predictions\"):\n",
        "                new_questions_found = True\n",
        "                yield question\n",
        "    else:\n",
        "        new_questions_found = True\n",
        "        yield from questions\n",
        "\n",
        "    if not response.json().get(\"next\"):\n",
        "      break\n",
        "    n += 1\n",
        "\n",
        "  if not new_questions_found:\n",
        "    print(\"No new questions to predict on.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "dXyQLerREJGk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "5919eca0-0ce5-48a7-e1d0-44ac98465b4d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "def find_number_before_percent(s):\n",
        "    # Use a regular expression to find all numbers followed by a '%'\n",
        "    matches = re.findall(r'Probability[:\\s]+(\\d+\\.?\\d*)%', s)\n",
        "    if matches:\n",
        "        # Return the last number found before a '%'\n",
        "        #return int(matches[-1])\n",
        "        return float(matches[-1])\n",
        "    else:\n",
        "        # Return None if no number found\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# this is used to replace the {} keys with [[]], since sometimes the LLM output uses {} when formatting code.\n",
        "def replace_keys(text, key_dict, delimiter='[[', end_delimiter=']]'):\n",
        "    pattern = re.compile(re.escape(delimiter) + '(.*?)' + re.escape(end_delimiter))\n",
        "    def replace(match):\n",
        "        key = match.group(1)\n",
        "        return key_dict.get(key, match.group(0))  # Return the original if key not found\n",
        "    return pattern.sub(replace, text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "ldAJ5OWBW1mA",
        "outputId": "404be223-7a86-4bd2-a6b0-ddc1a291dec2"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "EoV2KKC8l5im",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "4c51c1f8-1859-4a20-8f55-f3732d244273"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "def predict(question_id, prediction_percentage):\n",
        "  prediction_decimal = float(prediction_percentage) / 100.0\n",
        "  url = f\"https://www.metaculus.com/api2/questions/{question_id}/predict/\"\n",
        "  response = requests.post(\n",
        "      url,\n",
        "      json={\n",
        "        \"prediction\": prediction_decimal\n",
        "      },\n",
        "      headers={\"Authorization\": f\"Token {token}\"},\n",
        "  )\n",
        "  response.raise_for_status()\n",
        "  print(f\"Successfully predicted {prediction_percentage}% ({prediction_decimal}) on question {question_id}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "AJUGVfHVrms2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "9b0f6281-2f4c-49e3-aa51-ec2323c7ae14"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "def formulate_comment(prediction_json):\n",
        "  comment_blocks = []\n",
        "  if \"reasoning_base_rate\" in prediction_json:\n",
        "    comment_blocks.append(\"## Base rate estimation\")\n",
        "    comment_blocks.append(prediction_json[\"reasoning_base_rate\"])\n",
        "  if \"reasoning_reference_classes\" in prediction_json:\n",
        "    comment_blocks.append(\"## Reference classes\")\n",
        "    comment_blocks.append(prediction_json[\"reasoning_reference_classes\"])\n",
        "  if \"reasoning_other\" in prediction_json:\n",
        "    comment_blocks.append(\"## Additional\")\n",
        "    comment_blocks.append(prediction_json[\"reasoning_other\"])\n",
        "  return \"\\n\".join(comment_blocks) if comment_blocks else \"No reasoning provided\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "7uhKwbzQsKby",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "9f54361d-3b06-4b72-9ed4-01a2af6c9bbf"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "def comment(question_id, comment_text):\n",
        "\n",
        "  # for submit_type choose \"S\" to post regular comment and \"N\" for private. Tournament submissions should be private comments.\n",
        "  url = f\"https://www.metaculus.com/api2/comments/\"\n",
        "  response = requests.post(\n",
        "    url,\n",
        "    json={\n",
        "      \"comment_text\":comment_text,\"submit_type\":\"N\",\"include_latest_prediction\":True,\"question\":question_id\n",
        "    },\n",
        "    headers={\"Authorization\": f\"Token {token}\"},\n",
        "  )\n",
        "  response.raise_for_status()\n",
        "  print(\"Comment Success!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "T2PaTg5H6muR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "d3f9e023-57be-426f-a597-742f9776969b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#def get_perplexity_research(question_id):\n",
        "#  url = \"https://ml.metaculus.com/questions-api/perplexity-research-results/\"\n",
        "#  headers = {\n",
        "#    \"accept\": \"application/json\",\n",
        "#    \"X-API-Key\": questions_api_key,\n",
        "#    \"content-type\": \"application/json\"\n",
        "#  }\n",
        "#  params = {\n",
        "#    \"question_id\": question_id\n",
        "#  }\n",
        "#  response = requests.get(url=url, params=params, headers=headers)\n",
        "#  if response.status_code == 404:\n",
        "#    print(\"No Perplexity research found\")\n",
        "#    return \"No results found, please use your own knowledge and judgement to forecast\"\n",
        "#  content = response.text\n",
        "\n",
        "#  print(\"Generated research from perplexity:\")\n",
        "#  print(content)\n",
        "#  return content"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def estimate_pricing(input, output, model):\n",
        "  encoding = tiktoken.encoding_for_model(model)\n",
        "  input_len = len(encoding.encode(input))\n",
        "  output_len = len(encoding.encode(output))\n",
        "\n",
        "\n",
        "  # hard coding for now, maybe make it smarter later\n",
        "  # units of $ per token\n",
        "  gpt4o_input_pricing = 5 / 1_000_000\n",
        "  gpt4o_output_pricing = 15 / 1_000_000\n",
        "\n",
        "  input_cost = input_len * gpt4o_input_pricing\n",
        "  output_cost = output_len * gpt4o_output_pricing\n",
        "  total_cost = input_cost + output_cost\n",
        "\n",
        "  return input_cost, output_cost, total_cost"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "THGjV1tPd_7Q",
        "outputId": "acec86de-a9e7-48dc-cb06-38d9388f8b45"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def run_llm(today, client, template_values, prompt_template, summary_report, model, looking_for_probability=True):\n",
        "\n",
        "  title = template_values[\"title\"]\n",
        "  resolution_criteria = template_values[\"resolution_criteria\"]\n",
        "  background = template_values[\"description\"]\n",
        "  if template_values[\"fine_print\"]:\n",
        "    fine_print = template_values[\"fine_print\"]\n",
        "  else:\n",
        "    fine_print = \"none\"\n",
        "\n",
        "  prompt_dict = {\n",
        "      \"title\": title,\n",
        "      \"summary_report\": summary_report,\n",
        "      \"today\": today,\n",
        "      \"background\": background,\n",
        "      \"fine_print\": fine_print,\n",
        "      \"resolution_criteria\": resolution_criteria,\n",
        "  }\n",
        "\n",
        "  prompt_text = replace_keys(prompt_template, prompt_dict)\n",
        "\n",
        "  #print(\"Here is the prompt used:\")\n",
        "  #print(\"prompt_text\")\n",
        "  #print(prompt_text)\n",
        "  #print(\"\")\n",
        "\n",
        "  chat_completion = client.chat.completions.create(\n",
        "    model=model,\n",
        "    messages=[\n",
        "      {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": prompt_text\n",
        "      }\n",
        "    ]\n",
        "  )\n",
        "\n",
        "  gpt_text = chat_completion.choices[0].message.content\n",
        "\n",
        "  #estimate cost\n",
        "  input_cost, output_cost, total_cost = estimate_pricing(prompt_text, gpt_text, model)\n",
        "\n",
        "  if looking_for_probability:\n",
        "    # Regular expression to find the number following 'Probability: '\n",
        "    probability_match = find_number_before_percent(gpt_text)\n",
        "\n",
        "    # Extract the number if a match is found\n",
        "    if probability_match:\n",
        "        #probability = int(probability_match) # int(match.group(1))\n",
        "        probability = float(probability_match)\n",
        "        #print(f\"The extracted probability is: {probability}%\")\n",
        "        #probability = min(max(probability, 3), 97) # To prevent extreme forecasts\n",
        "    else:\n",
        "        probability = None\n",
        "        print(\"No probability found in the text! Skipping!\")\n",
        "  else:\n",
        "    probability = None\n",
        "  return probability, gpt_text, input_cost, output_cost, total_cost"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "XK4FbnKJb98Q",
        "outputId": "98b6c6c4-f4e7-4e34-e69a-9f24accbf3b3"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def call_perplexity(perplexity_prompt, perplexity_api_key):\n",
        "\n",
        "  from openai import OpenAI\n",
        "\n",
        "  YOUR_API_KEY = perplexity_api_key\n",
        "\n",
        "  messages = [\n",
        "      {\n",
        "          \"role\": \"system\",\n",
        "          \"content\": (\n",
        "              \"You are an artificial intelligence assistant and you need to \"\n",
        "              \"engage in a helpful, detailed, polite conversation with a user.\"\n",
        "          ),\n",
        "      },\n",
        "      {\n",
        "          \"role\": \"user\",\n",
        "          \"content\": (\n",
        "              perplexity_prompt\n",
        "          ),\n",
        "      },\n",
        "  ]\n",
        "\n",
        "  perplexity_client = OpenAI(api_key=YOUR_API_KEY, base_url=\"https://api.perplexity.ai\")\n",
        "\n",
        "  # chat completion without streaming\n",
        "  response = perplexity_client.chat.completions.create(\n",
        "      model=PERPLEXITY_MODEL,\n",
        "      messages=messages,\n",
        "  )\n",
        "\n",
        "  content = response.choices[0].message.content\n",
        "\n",
        "  print(\"Generated research from perplexity:\")\n",
        "  print(content)\n",
        "  print(\"\")\n",
        "\n",
        "  # get token and cost estimate\n",
        "\n",
        "  # currently using the GPT tokenizer with a 1.3 multiplier. Hacky and wrong, but rough estimate.\n",
        "  # See here for 1.3 factor estimate source: https://github.com/continuedev/continue/issues/878\n",
        "\n",
        "  perplexity_token_pricing = 1/1_000_000\n",
        "  perplexity_cost_fixed = 5/1_000\n",
        "\n",
        "  multiplier = 1.3\n",
        "  encoding = tiktoken.encoding_for_model(OPEN_AI_MODEL)\n",
        "  input_text = perplexity_prompt\n",
        "  output_text = content\n",
        "  input_len = len(encoding.encode(input_text)) * multiplier\n",
        "  output_len = len(encoding.encode(output_text)) * multiplier\n",
        "\n",
        "  input_cost = input_len * perplexity_token_pricing\n",
        "  output_cost = output_len * perplexity_token_pricing\n",
        "  fixed_cost = perplexity_cost_fixed\n",
        "  total_cost = input_cost + output_cost + perplexity_cost_fixed\n",
        "\n",
        "  print(f\"Total perplexity call cost: ${total_cost}\")\n",
        "  print(\"\")\n",
        "\n",
        "  return content, total_cost"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "gWXOgDDWx7il",
        "outputId": "dfc1e597-6995-4da8-d8cc-20b470ae83de"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "W-wVKhWomV2C",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "17c978af-f8e5-4ae4-d9f0-0b417851269a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "def clean_gpt_turbo_markdown(text: str) -> str:\n",
        "  match = re.search(r\"```[\\w]+\\s+(.*?)\\s+```\", text, re.DOTALL)\n",
        "  if match:\n",
        "    cleaned_text = match.group(1).strip()\n",
        "  else:\n",
        "    cleaned_text = text\n",
        "  return cleaned_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwc6I64ScFUz"
      },
      "source": [
        "Forecast on all questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "3C52dUgSG6Pt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9bf8234a-8779-4ea4-c7ff-8ca448eb960f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now Forecasting Question:\n",
            "None Will a nuclear weapon be detonated as an act of war by Sept 30, 2024?\n",
            "\n",
            "Getting Perplexity recent data...\n",
            "\n",
            "The completed question posed to perplexity reads: What is the most recent news available and prior occurrences related to \"nuclear threats or tensions worldwide?\"\n",
            "Generated research from perplexity:\n",
            "The most recent news available and prior occurrences related to \"nuclear threats or tensions worldwide\" include:\n",
            "\n",
            "1. **Russia's Nuclear Rhetoric and Exercises**:\n",
            "   - Russia has been escalating its nuclear rhetoric and conducting military exercises, including drills for the potential use of non-strategic nuclear weapons in the context of the ongoing war in Ukraine.\n",
            "   - These developments are concerning but not surprising, as Russia typically invokes strong rhetoric during its annual \"Victory Day\" celebrations.\n",
            "\n",
            "2. **China and Russia's Joint Statement**:\n",
            "   - Chinese President Xi Jinping and Russian President Vladimir Putin issued a joint statement emphasizing that \"there can be no winners in a nuclear war and it should never be fought,\" echoing a similar statement made by President Ronald Reagan and Soviet Leader Mikhail Gorbachev in 1985.\n",
            "\n",
            "3. **U.S. Calls for AI Regulation in Nuclear Deployment**:\n",
            "   - The U.S. has called on China and Russia to join the United States, France, and Britain in declaring that only humans, not Artificial Intelligence (AI), will make decisions on nuclear deployment.\n",
            "\n",
            "4. **North Korea's Nuclear Drills and Diplomacy**:\n",
            "   - North Korea conducted a military drill simulating a nuclear counterattack, testing a new nuclear command-and-control system called \"Haekbangashoe\" that can switch the dual-capable KN-25 ballistic missiles from non-nuclear to nuclear missions on short notice.\n",
            "   - North Korea also sent a high-level diplomatic delegation to Iran, marking the first such engagement since 2019.\n",
            "\n",
            "5. **Iranian Nuclear Threats**:\n",
            "   - Iranian leaders have been threatening to construct nuclear bombs, with some alluding to already having one, following military exchanges with Israel.\n",
            "\n",
            "6. **Global Nuclear Arsenals and Modernization**:\n",
            "   - The nine nuclear-armed states (the United States, Russia, the United Kingdom, France, China, India, Pakistan, North Korea, and Israel) continued to modernize their nuclear arsenals, with several deploying new nuclear-armed or nuclear-capable weapon systems in 2023.\n",
            "   - The global total of nuclear warheads continues to fall, but operational nuclear warheads are increasing, with Russia and the USA possessing almost 90% of all nuclear weapons.\n",
            "\n",
            "7. **Nuclear Diplomacy and Treaties**:\n",
            "   - Nuclear arms control and disarmament diplomacy have suffered setbacks, including Russia's suspension of its participation in the New START treaty and its withdrawal from the Comprehensive Nuclear-Test-Ban Treaty (CTBT).\n",
            "   - The United States and China have held nuclear risk reduction talks, but China recently declined to resume these talks.\n",
            "\n",
            "These developments highlight the ongoing tensions and concerns surrounding nuclear threats and the need for continued diplomatic efforts to reduce the risks of nuclear conflict.\n",
            "\n",
            "Total perplexity call cost: $0.0057540000000000004\n",
            "\n",
            "Running initial prompt 1\n",
            "Output Reasoning:\n",
            "Based on the current situation and several factors that are under consideration, the probability of a nuclear weapon be detonated as an act of war by Sept 30, 2024, can be estimated.\n",
            "\n",
            "1. **Historical Context**: Since the dropping of nuclear bombs by the United States on Japan during World War II in 1945, there have been no instances of nuclear weapons being used in warfare. This suggests a historical and cultural reluctance to utilize nuclear weapons.\n",
            "\n",
            "2. **Mutually Assured Destruction (MAD)**: Most nuclear-armed states recognize that the use of nuclear weapons in warfare would lead to catastrophic and mutually assured destruction. This belief has been a significant deterrent in the past.\n",
            "\n",
            "3. **Diplomacy and Treaties**: Despite recent troubles in nuclear diplomacy, the fact that nations are still engaging in dialogue and negotiation suggests they are seeking to avoid direct conflict, especially nuclear conflict. While Russia's withdrawal from the New START treaty and CTBT is concerning, it doesn't necessarily indicate imminent warfare.\n",
            "\n",
            "4. **Nuclear Posturing**: While the rhetoric and posturing from nations such as Russia, North Korea, and Iran are concerning, it's well recognized that this can often be a form of 'saber-rattling' or a tool for domestic or international political gains rather than a strong predictive element for nuclear warfare.\n",
            "\n",
            "5. **Modernization of Nuclear Arsenals**: The continued development and modernization of nuclear arsenal around the world is a worrying trend. But, it is also a part of the nuclear arms race and strategic tactics to maintain or gain global leadership rather than an indicator of an impending nuclear war.\n",
            "\n",
            "6. **AI and Nuclear Delegation**: With U.S. emphasizing about the AI Regulation in Nuclear Deployment, it seems there is thought given to handling nuclear arsenals in a more controlled, precise manner to avoid accidental launches or unwarranted decisions. \n",
            "\n",
            "Given the above considerations, while it is clear that the threat of nuclear conflict does exist and it is not completely non-existent, based on historical reluctance, deterrents due to MAD, continuing diplomatic efforts, and differentiation between nuclear posturing and actual conflict indicates it's still a low probability event. \n",
            "\n",
            "**Probability: 1.0%**\n",
            "\n",
            "Extracted probability percentage value (0-100): 1.0\n",
            "\n",
            "~~~~ NEXT PROMPT ~~~~\n",
            "\n",
            "Running initial prompt 2\n",
            "Output Reasoning:\n",
            "With all the given information, several key steps come in to formulate a forecast in terms of the probability of a nuclear weapon being detonated as an act of war by September 30, 2024. The greatly complex and sensitive nature of nuclear warfare calls for a rigorous examination of the geopolitical context, relevant historical precedents, and current developments. \n",
            "\n",
            "1. **Base Rate (historical precedent)**: The most appropriate starting point for this forecast is the historical precedent - the base rate of nuclear weapons being used in an act of war. After the use of atomic bombs on Hiroshima and Nagasaki in 1945 (World War II), no nuclear weapons have been used in active warfare. This gives a very low base rate for nuclear warfare in the seven decades since 1945. \n",
            "\n",
            "2. **Recent Developments and Tensions**: The information provided by the research assistant points towards serious global nuclear threats, mostly pertaining to Russia, China, North Korea, Iran, and the U.S. Increased military exercises, nuclear rhetoric, and modernization of nuclear arsenals form the basis of these nuclear tensions. However, it is equally important to note that joint statements emphasizing the avoidance of nuclear wars (as seen from China and Russia), negotiations, and diplomatic engagements are also ongoing simultaneously.\n",
            "\n",
            "3. **Geopolitical Context and Nuclear Deterrence Theory**: It's crucial to consider one of the main principles of geopolitics: nuclear deterrence theory that dissuades states from using nuclear weapons because of the threat of mutually assured destruction. This principle has played a major role in preventing nuclear war despite episodes of high tension such as the Cuban Missile Crisis. \n",
            "\n",
            "4. **Time Frame**: The fact that the question asks about a nuclear weapon detonation by September 30, 2024, which is just two months and some days away from today (2024-07-20), is a significantly small window of time for a nuclear warfare escalation given the usual trajectories of international disputes and diplomatic efforts around nuclear issues. \n",
            "\n",
            "5. **Impact of AI on Nuclear Warfare**: The mention of the U.S. calling for AI regulations in nuclear deployment signals a paradigmatic shift in methods of warfare, but it's not clear whether or how it would necessarily expedite probabilities of a nuclear detonation. \n",
            "\n",
            "In light of these considerations, we can say that while nuclear threats and overall tensions have seemingly increased in recent years, the deterrent effect of mutually assured destruction, the ongoing diplomatic engagements, and the very short timeframe offered by the question, combined with the historical base rate, suggest that it's still highly unlikely that a nuclear weapon will be detonated by September 30, 2024.\n",
            "\n",
            "Given all these factors, I arrive at the following probability:\n",
            "\n",
            "\"Probability: 1.0%\"\n",
            "\n",
            "Extracted probability percentage value (0-100): 1.0\n",
            "\n",
            "~~~~ NEXT PROMPT ~~~~\n",
            "\n",
            "Running initial prompt 3\n",
            "Output Reasoning:\n",
            "Given the context and through analyzing the available data, we can make certain deductions in order to reach a forecast. \n",
            "\n",
            "1. **Russia's Nuclear Rhetoric and Exercises**: The escalated rhetoric and military exercises by Russia can be alarming yet it is essential to consider that this kind of show of military strength is not unprecedented, especially during their annual celebrations, thus it does not provide strong evidence for an imminent nuclear war launching. \n",
            "\n",
            "2. **China and Russia's Joint Statement**: The joint statement made by the leaders of Russia and China communicates a clear indication against nuclear war. This underlines that the biggest nuclear superpowers are not likely to engage in nuclear warfare which will reflect on the behavior of other nuclear states.\n",
            "\n",
            "3. **U.S. Calls for AI Regulation in Nuclear Deployment**: The advocacy by US for regulations on AI in nuclear deployment suggests the existence of defensive strategies and strong security measures which will potentially lower the chances of an unexpected nuclear war taking place. \n",
            "\n",
            "4. **North Korea's Nuclear Drills and Diplomacy**: While North Korea's testing of nuclear systems is concerning, it is also essential to consider their bouts of diplomatic engagements which might be an indication that they are not planning to immediate unleash nuclear war. \n",
            "\n",
            "5. **Iranian Nuclear Threats**: Iran's threats are worrying, yet they must be looked at in perspective of the overall international landscape which generally discourages nuclear warfare. \n",
            "\n",
            "6. **Global Nuclear Arsenals and Modernization**: The continued modernization of nuclear arsenals does not necessarily point towards the imminent risk of a nuclear war. \n",
            "\n",
            "7. **Nuclear Diplomacy and Treaties**: The setbacks in nuclear diplomacy could be seen as a red flag, yet, it is noteworthy that such changes are part of the diplomacy game and more complex negotiations could follow. \n",
            "\n",
            "Given these analyses, the chance of a nuclear war, specifically the detonation of a nuclear weapon, seems quite low based on the current data, yet not completely negligible considering the potential tensions and unpredictable global politics.\n",
            "\n",
            "So, taking into account that I should be confident in my forecast and extremize towards 1% or 99%, and that departures from the norm are less likely, my final forecast is:\n",
            "\n",
            "Probability: 1.4%.\n",
            "\n",
            "Extracted probability percentage value (0-100): 1.4\n",
            "\n",
            "~~~~ NEXT PROMPT ~~~~\n",
            "\n",
            "+++++++++++ FINAL PROMPT (4) +++++++++++++++++\n",
            "Running final prompt...\n",
            "\n",
            "Summary of Forecaster A's reasoning: There exists a low probability (1.0%) of a nuclear warfare event driven by historical reluctance to use such weapons, established principle of Mutually Assured Destruction (MAD), ongoing diplomatic efforts, and separating nuclear posturing from genuine intent of conflict.\n",
            "\n",
            "Summary of Forecaster B's reasoning: The probability is 1.0% as per this forecaster who also considers historical precedents, recent nuclear-related activities and tensions, the geopolitical context including the theory of nuclear deterrence, the short timeframe of the question, and the uncertain impact of AI on nuclear warfare.\n",
            "\n",
            "Summary of Forecaster C's reasoning: With a slight increase in estimated probability (1.4%), this forecaster considers Russia's nuclear rhetoric practices, joint statements from nuclear superpowers, regulations on AI in nuclear deployment, continued diplomatic engagements by generally aggressive states like North Korea, threats from Iran, continued nuclear arsenal modernization, and changes in nuclear diplomacy.\n",
            "\n",
            "In synthesizing these forecasts, I would align more with the forecaster A's and B's approach. Despite current nuclear threats and tensions, historical data plays a significant role in this forecast. Since the end of World War II, there have been no instances of nuclear weapons being used in warfare. This lack of historical precedent is a powerful indicator of future probabilities making a strong case for a very low base rate.\n",
            "\n",
            "Forecaster B's emphasis on the short timeframe till September 30, 2024 is a valid point to consider as well. Major escalations in nuclear hostility tend to involve extensive periods of diplomatic combat prior to any militaristic action. \n",
            "\n",
            "Forecaster C's point about the use of AI in nuclear deployment also adds a new dimension to nuclear war forecasts. If regulations on AI are successfully developed and implemented in nuclear warcraft, the probability of an accidental nuclear detonation can be potentially reduced. However, it's still an evolving situation and its real impact on this forecast cannot be fully quantified. \n",
            "\n",
            "The geopolitical and nuclear context of today does warrant a recognition of the existing nuclear threats. The principle of MAD, although not perfect, has deterred nations successfully from using nuclear weapons in the past. Recent joint statements reemphasizing non-usage of nuclear weapons are positive signals that should be taken into account.\n",
            "\n",
            "Therefore, my forecast will align closer to the lower percentages predicted by Forecasters A and B. However, given the somewhat higher-than-normal global tensions about nuclear threats, I will conservatively adjust my probability slightly above their estimates.\n",
            "\n",
            "Probability: 1.5%.\n",
            "\n",
            "Final prompt's individual (non-weighted) extracted probability percentage value (0-100): 1.5\n",
            "\n",
            "Rationale to submit:\n",
            "\n",
            "\n",
            "  *This forecast is produced from several prompts initially making forecasts, then a summary forecaster looks over all the previous forecasts and rationales and assess them for level of convincingness before giving its own forecast.  Then a weighted forecast is produced by combining all the inidividual and summary forecasts, weighted based on the initial forecasters' reported confidence and an independently assessed level of rationale strength.  The individual forecasts are reasoned based on knowledge stored inside the latest OpenAI GPT models, supplemented with recent information from targeted Perplexity search results.*\n",
            "\n",
            "  * *Main LLM Model used: gpt-4*\n",
            "  * *Weighted formula: (0.2)(Forecaster A) + (0.2)(Forecaster B) + (0.2)(Forecaster C) + (0.4)(Summary Forecaster)*\n",
            "  * *Estimated cost of model calls based on tokens (may be incorrect due to differing token costs based on model used): $0.056*\n",
            "  * *** FINAL WEIGHTED FORECAST: 1.3%***\n",
            "\n",
            "  ---\n",
            "\n",
            "  \n",
            "\n",
            "Output Reasoning:\n",
            "Summary of Forecaster A's reasoning: There exists a low probability (1.0%) of a nuclear warfare event driven by historical reluctance to use such weapons, established principle of Mutually Assured Destruction (MAD), ongoing diplomatic efforts, and separating nuclear posturing from genuine intent of conflict.\n",
            "\n",
            "Summary of Forecaster B's reasoning: The probability is 1.0% as per this forecaster who also considers historical precedents, recent nuclear-related activities and tensions, the geopolitical context including the theory of nuclear deterrence, the short timeframe of the question, and the uncertain impact of AI on nuclear warfare.\n",
            "\n",
            "Summary of Forecaster C's reasoning: With a slight increase in estimated probability (1.4%), this forecaster considers Russia's nuclear rhetoric practices, joint statements from nuclear superpowers, regulations on AI in nuclear deployment, continued diplomatic engagements by generally aggressive states like North Korea, threats from Iran, continued nuclear arsenal modernization, and changes in nuclear diplomacy.\n",
            "\n",
            "In synthesizing these forecasts, I would align more with the forecaster A's and B's approach. Despite current nuclear threats and tensions, historical data plays a significant role in this forecast. Since the end of World War II, there have been no instances of nuclear weapons being used in warfare. This lack of historical precedent is a powerful indicator of future probabilities making a strong case for a very low base rate.\n",
            "\n",
            "Forecaster B's emphasis on the short timeframe till September 30, 2024 is a valid point to consider as well. Major escalations in nuclear hostility tend to involve extensive periods of diplomatic combat prior to any militaristic action. \n",
            "\n",
            "Forecaster C's point about the use of AI in nuclear deployment also adds a new dimension to nuclear war forecasts. If regulations on AI are successfully developed and implemented in nuclear warcraft, the probability of an accidental nuclear detonation can be potentially reduced. However, it's still an evolving situation and its real impact on this forecast cannot be fully quantified. \n",
            "\n",
            "The geopolitical and nuclear context of today does warrant a recognition of the existing nuclear threats. The principle of MAD, although not perfect, has deterred nations successfully from using nuclear weapons in the past. Recent joint statements reemphasizing non-usage of nuclear weapons are positive signals that should be taken into account.\n",
            "\n",
            "Therefore, my forecast will align closer to the lower percentages predicted by Forecasters A and B. However, given the somewhat higher-than-normal global tensions about nuclear threats, I will conservatively adjust my probability slightly above their estimates.\n",
            "\n",
            "Probability: 1.5%.\n",
            "\n",
            "FINAL WEIGHTED FORECAST:\n",
            "1.3%\n",
            "\n",
            "Overall cost was: $0.055915000000000006\n",
            "\n",
            "################ NEXT QUESTION #################\n",
            "\n",
            "Finished forecasting on 1 questions.\n"
          ]
        }
      ],
      "source": [
        "today = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
        "client = OpenAI()\n",
        "model = OPEN_AI_MODEL\n",
        "ENABLE_PERPLEXITY_RESEARCH = False  # Previously this could be set to True to get and use pre-computed Perplexity research results for the question, set to False otherwise.\n",
        "# However, Metaculus no longer supports pre-computed Perplexity research, so it has been permanently set to False here to skip over that setp.\n",
        "\n",
        "prompt_templates = [prompt_template_v1 + QUESTION_INFO_TEMPLATE, prompt_template_v2 + QUESTION_INFO_TEMPLATE, prompt_template_v3 + QUESTION_INFO_TEMPLATE]\n",
        "\n",
        "forecasted_count = 0\n",
        "\n",
        "for question_to_forecast_dict in yield_all_questions():\n",
        "  if forecasted_count >= MAX_QUESTIONS_TO_FORECAST:\n",
        "    break\n",
        "  if QUESTION_IDS_TO_FORECAST is not None and question_to_forecast_dict[\"id\"] not in QUESTION_IDS_TO_FORECAST:\n",
        "    continue\n",
        "\n",
        "  print(\"Now Forecasting Question:\")\n",
        "  print(question_to_forecast_dict[\"id\"], question_to_forecast_dict[\"title\"])\n",
        "  print(\"\")\n",
        "\n",
        "  #define perplexity research to use\n",
        "  perplexity_total_cost = \"N/A\"\n",
        "\n",
        "  #if ENABLE_PERPLEXITY_RESEARCH:\n",
        "  #  summary_report = get_perplexity_research(question_to_forecast[\"id\"])\n",
        "  #else:\n",
        "  #  #summary_report = \"No results found, please use your own knowledge and judgement to forecast\"\n",
        "  #  summary_report = \"\"\n",
        "  summary_report = \"\" # ?\n",
        "\n",
        "  # set summary_report to the perplexity recent search if enabled\n",
        "  if USE_PERPLEXITY_RECENT:\n",
        "    print(\"Getting Perplexity recent data...\")\n",
        "    print (\"\")\n",
        "    #print(\"run_llm(...):\")\n",
        "    #print(\"today\", today)\n",
        "    #print(\"client\", client)\n",
        "    #print(\"question_to_forecast_dict\", question_to_forecast_dict)\n",
        "    #print(\"prompt_template_get_perplexity_question\", prompt_template_get_perplexity_question)\n",
        "    #print(\"summary_report\", summary_report)\n",
        "    #print(\"model\", model)\n",
        "\n",
        "    #get prompt completion for use with perplexity\n",
        "    probability, perplexity_recent_prompt_completion, completion_input_cost, completion_output_cost, completion_total_cost = run_llm(today, client, question_to_forecast_dict, prompt_template_get_perplexity_question, summary_report, model, looking_for_probability=False)\n",
        "    #print(\"perplexity_recent_prompt_completion\", perplexity_recent_prompt_completion)\n",
        "    #print(\"completion_input_cost\", completion_input_cost)\n",
        "    #print(\"completion_output_cost\", completion_output_cost)\n",
        "    #print(\"completion_total_cost\", completion_total_cost)\n",
        "    #print(\"\")\n",
        "\n",
        "    perplexity_recent_prompt = \"What is the most recent news available and prior occurrences related to \" + perplexity_recent_prompt_completion\n",
        "\n",
        "    print(f\"The completed question posed to perplexity reads: {perplexity_recent_prompt}\")\n",
        "    #get recent news from perplexity\n",
        "    #print(\"call_perplexity(...)\")\n",
        "    perplexity_content, perplexity_cost = call_perplexity(perplexity_recent_prompt, perplexity_api_key)\n",
        "\n",
        "    summary_report = perplexity_content\n",
        "    perplexity_total_cost = completion_total_cost + perplexity_cost\n",
        "\n",
        "  # need to iterate through prompts here\n",
        "  all_forecasts = []\n",
        "  overall_cost = 0\n",
        "\n",
        "  i = 0\n",
        "  for prompt_template in prompt_templates:\n",
        "    i+=1\n",
        "    print(\"Running initial prompt %s\" % i)\n",
        "    #print(\"run_llm(...)\")\n",
        "    probability, gpt_text, input_cost, output_cost, total_cost = run_llm(today, client, question_to_forecast_dict, prompt_template, summary_report, model)\n",
        "    all_forecasts.append((probability, gpt_text, input_cost, output_cost, total_cost))\n",
        "    overall_cost += total_cost\n",
        "    print(f\"Output Reasoning:\")\n",
        "    print(gpt_text)\n",
        "    print(\"\")\n",
        "    print(\"Extracted probability percentage value (0-100): %s\" % probability)\n",
        "    #print(f\"Input cost: ${input_cost}\")\n",
        "    #print(f\"Output cost: ${output_cost}\")\n",
        "    #print(f\"Total cost: ${total_cost}\")\n",
        "    #print(f\"Perplexity search costs (including LLM prompt completion): ${completion_total_cost + perplexity_cost}\")\n",
        "    print(\"\")\n",
        "    print(\"~~~~ NEXT PROMPT ~~~~\")\n",
        "    print(\"\")\n",
        "\n",
        "  final_prompt_part2_dict = {\n",
        "      \"forecaster1\": all_forecasts[0][1],\n",
        "      \"forecaster2\": all_forecasts[1][1],\n",
        "      \"forecaster3\": all_forecasts[2][1],\n",
        "  }\n",
        "\n",
        "  formatted_final_prompt_part2 = replace_keys(final_prompt_part2, final_prompt_part2_dict)\n",
        "\n",
        "  print(\"+++++++++++ FINAL PROMPT (4) +++++++++++++++++\")\n",
        "\n",
        "  final_prompt_template = final_prompt_part1 + QUESTION_INFO_TEMPLATE + formatted_final_prompt_part2\n",
        "\n",
        "  print(\"Running final prompt...\")\n",
        "\n",
        "  probability, gpt_text, input_cost, output_cost, total_cost = run_llm(today, client, question_to_forecast_dict, final_prompt_template, summary_report, model)\n",
        "\n",
        "  print(\"\")\n",
        "  print(gpt_text)\n",
        "  print(\"\")\n",
        "\n",
        "  print(\"Final prompt's individual (non-weighted) extracted probability percentage value (0-100): %s\" % probability)\n",
        "  print(\"\")\n",
        "\n",
        "  forecaster1_weight = 0.2\n",
        "  forecaster2_weight = 0.2\n",
        "  forecaster3_weight = 0.2\n",
        "  forecaster4_weight = 0.4\n",
        "\n",
        "  weighted_forecast = forecaster1_weight*float(all_forecasts[0][0]) + forecaster2_weight*float(all_forecasts[1][0]) + forecaster3_weight*float(all_forecasts[2][0]) + forecaster4_weight*float(probability)\n",
        "  weighted_forecast = round(weighted_forecast,1)\n",
        "  overall_cost = overall_cost + total_cost\n",
        "\n",
        "  #create summary strings for comments:\n",
        "  header_string = f\"\"\"\n",
        "  *This forecast is produced from several prompts initially making forecasts, then a summary forecaster looks over all the previous forecasts and rationales and assess them for level of convincingness before giving its own forecast.  Then a weighted forecast is produced by combining all the inidividual and summary forecasts, weighted based on the initial forecasters' reported confidence and an independently assessed level of rationale strength.  The individual forecasts are reasoned based on knowledge stored inside the latest OpenAI GPT models, supplemented with recent information from targeted Perplexity search results.*\n",
        "\n",
        "  * *Main LLM Model used: {model}*\n",
        "  * *Weighted formula: ({forecaster1_weight})(Forecaster A) + ({forecaster2_weight})(Forecaster B) + ({forecaster3_weight})(Forecaster C) + ({forecaster4_weight})(Summary Forecaster)*\n",
        "  * *Estimated cost of model calls based on tokens (may be incorrect due to differing token costs based on model used): ${round(overall_cost,3)}*\n",
        "  * *** FINAL WEIGHTED FORECAST: {weighted_forecast}%***\n",
        "\n",
        "  ---\n",
        "\n",
        "  \"\"\"\n",
        "  print('Rationale to submit:')\n",
        "  print(\"\")\n",
        "  print(header_string)\n",
        "  print(\"\")\n",
        "\n",
        "  forecasted_count += 1\n",
        "  if SUBMIT_FORECASTS and weighted_forecast is not None:\n",
        "    predict(question_to_forecast_dict[\"id\"], float(weighted_forecast))\n",
        "    comment(question_to_forecast_dict[\"id\"], header_string + \"PERPLEXITY\\n\\n\" + summary_report + \"\\n\\n---\\n\\n\" + \"GPT\\n\\n\" + gpt_text)\n",
        "\n",
        "  print(f\"Output Reasoning:\")\n",
        "  print(gpt_text)\n",
        "  print(\"\")\n",
        "  print(\"FINAL WEIGHTED FORECAST:\")\n",
        "  print(f\"{weighted_forecast}%\")\n",
        "  print(\"\")\n",
        "  print(f\"Overall cost was: ${overall_cost}\")\n",
        "  print(\"\")\n",
        "  print(\"################ NEXT QUESTION #################\")\n",
        "  print(\"\")\n",
        "\n",
        "print(\"Finished forecasting on %s questions.\" % forecasted_count)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6qfSjn0L_8I3"
      },
      "execution_count": 83,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}